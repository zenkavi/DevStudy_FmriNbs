{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(alpha, beta, n=100, \n",
    "                  p_r={'high_var': [.95, .05], 'low_var': [.5,.5]},\n",
    "                  rs = np.array(([5.0, -495.0],[-5.0, 495.0],[10.0, -100.0],[-10.0, 100.0])),\n",
    "                  sQ = np.zeros((4, 2))\n",
    "                 ):\n",
    "    \n",
    "    # Need to denote both machine type and action\n",
    "    \n",
    "    # Pre-specify machines for each trial in a randomly balanced manner\n",
    "    if n%4 != 0:\n",
    "        print(\"Number of trials is not divisable by 4.\\nCreating trials for %s trials.\"%(str(n-(n%4))))\n",
    "        n = n-(n%4)\n",
    "    \n",
    "    machs = np.array([0,1,2,3])\n",
    "    machs = np.tile(machs, int(n/4))\n",
    "    np.random.shuffle(machs)\n",
    "    \n",
    "    # Initialize empty array that will be populated in the loop based on Q values\n",
    "    acts = np.zeros(n, dtype=np.int)\n",
    "    \n",
    "    # Generate by coin flip for machine with differing probabilities and outcomes\n",
    "    rews = np.zeros(n, dtype=np.int)\n",
    "\n",
    "    # Stores the expected value for each of 4 machines in each trial for each action\n",
    "    Qs = np.zeros((n, 4, 2))\n",
    "\n",
    "    # Initialize Q table\n",
    "    # Denotes expected value of each action\n",
    "    # Should look like [0, 0] for each machine\n",
    "    # *** The expected value of not playing should not change from 0! ***\n",
    "    # Could these initial expected values/beliefs also be estimated from data?\n",
    "    # E.g. what if kids have more optimistic priors about each machine though they learn at the same rate\n",
    "    Q = sQ.copy()\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        cur_machine = machs[i]\n",
    "        \n",
    "        # Apply the Softmax transformation\n",
    "        exp_Q = np.exp(beta*Q[cur_machine])\n",
    "        prob_a = exp_Q / np.sum(exp_Q)\n",
    "\n",
    "        # Simulate choice\n",
    "        a = np.random.choice([0, 1], p=prob_a)\n",
    "        \n",
    "        # Simulate reward if machine is played\n",
    "        if a == 1:\n",
    "    \n",
    "            # Before sampling reward determine which variance condition machine is in\n",
    "            if cur_machine>1:\n",
    "                cur_p = 'low_var'\n",
    "            else:\n",
    "                cur_p = 'high_var'\n",
    "\n",
    "            # Sample reward for current machine given its reward probs and outcome options\n",
    "            r = np.random.choice(rs[cur_machine], p = p_r[cur_p]) \n",
    "            \n",
    "            # Update Q table only if the machine is played\n",
    "            # And only the value of playing NOT of not playing\n",
    "            Q[cur_machine][a] = Q[cur_machine][a] + alpha * (r - Q[cur_machine][a])\n",
    "        \n",
    "        # If the machine is not played then Q remains unchanged and no reward is received\n",
    "        else:\n",
    "            r = 0.0\n",
    "\n",
    "        # Store values\n",
    "        acts[i] = a\n",
    "        rews[i] = r\n",
    "        #Qs[i] = Q.copy()\n",
    "        Qs[i] = Q\n",
    "\n",
    "    return machs, acts, rews, Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llik_td(x, *args):\n",
    "    # Extract the arguments as they are passed by scipy.optimize.minimize\n",
    "    alpha, beta = x\n",
    "    machines, actions, rewards = args\n",
    "\n",
    "    # Initialize values\n",
    "    Q = np.zeros((4, 2))\n",
    "    log_prob_actions = np.zeros(len(actions))\n",
    "\n",
    "    for t, (m, a, r) in enumerate(zip(machines, actions, rewards)):\n",
    "        \n",
    "        # Apply the softmax transformation\n",
    "        Q_ = Q[m] * beta\n",
    "        log_prob_action = Q_ - scipy.special.logsumexp(Q_)\n",
    "\n",
    "        # Store the log probability of the observed action\n",
    "        log_prob_actions[t] = log_prob_action[a]\n",
    "\n",
    "        # Update the Q values for the next trial\n",
    "        # Q[a] = Q[a] + alpha * (r - Q[a])\n",
    "        Q[m][a] = Q[m][a] + alpha * (r - Q[m][a])\n",
    "\n",
    "    # Return the negative log likelihood of all observed actions\n",
    "    return -np.sum(log_prob_actions[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_alpha = .3\n",
    "true_beta = .5\n",
    "n = 120\n",
    "machines, actions, rewards, all_Qs = generate_data(true_alpha, true_beta, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.588324993517084"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llik_td([true_alpha, true_beta], *(machines, actions, rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: 21.716473138069635\n",
      " hess_inv: array([[ 0.13369853, -1.62111246],\n",
      "       [-1.62111246, 19.77512604]])\n",
      "      jac: array([-1.66893005e-06, -4.76837158e-07])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 112\n",
      "      nit: 23\n",
      "     njev: 28\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([0.08698377, 1.23184674])\n",
      "\n",
      "MLE: alpha = 0.09 (true value = 0.3)\n",
      "MLE: beta = 1.23 (true value = 0.5)\n"
     ]
    }
   ],
   "source": [
    "x0 = [true_alpha, true_beta]\n",
    "#x0 = [.1, 3]\n",
    "result = scipy.optimize.minimize(llik_td, x0, args=(machines, actions, rewards), method='BFGS')\n",
    "print(result)\n",
    "print('')\n",
    "print(f'MLE: alpha = {result.x[0]:.2f} (true value = {true_alpha})')\n",
    "print(f'MLE: beta = {result.x[1]:.2f} (true value = {true_beta})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llik_td_vectorized(x, *args):\n",
    "    # Extract the arguments as they are passed by scipy.optimize.minimize\n",
    "    alpha, beta = x\n",
    "    machines, actions, rewards = args\n",
    "    n = len(actions)\n",
    "\n",
    "    # Create a list with the Q values of each trial\n",
    "    #Qs = np.ones((n, 2), dtype=np.float)\n",
    "    #Qs[0] = .5\n",
    "    Qs = np.zeros((n, 4, 2), dtype=np.float)\n",
    "    \n",
    "    # The last Q values were never used, so there is no need to compute them\n",
    "    for t, (m, a, r) in enumerate(zip(machines[:-1], actions[:-1], rewards[:-1])):\n",
    "        Qs[t] = Qs[t-1]\n",
    "        Qs[t, m, a] = Qs[t-1, m, a] + alpha * (r - Qs[t-1, m, a])\n",
    "        Qs[t, m, 1-a] = Qs[t-1, m, 1-a]\n",
    "\n",
    "    # Apply the softmax transformation in a vectorized way\n",
    "    idx = list(zip(range(n-1),machines[:-1], actions[:-1]))\n",
    "    obs_Qs = [Qs[i] for i in idx]\n",
    "    Qs_ = np.array(obs_Qs) * beta\n",
    "    log_prob_actions = Qs_ - scipy.special.logsumexp(Qs_, axis=1)[:, None]\n",
    "\n",
    "    # Return the log_prob_actions for the observed actions\n",
    "    log_prob_actions = log_prob_actions[np.arange(len(actions)), actions]\n",
    "    return -np.sum(log_prob_actions[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5557.576891122227"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llik_td_vectorized([true_alpha, true_beta], *(machines, actions, rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = true_alpha\n",
    "beta = true_beta\n",
    "n = 10\n",
    "Qs = np.zeros((n, 4, 2), dtype=np.float)\n",
    "    \n",
    "# The last Q values were never used, so there is no need to compute them\n",
    "for t, (m, a, r) in enumerate(zip(machines[:n], actions[:n], rewards[:n])):  \n",
    "    Qs[t] = Qs[t-1]\n",
    "    Qs[t, m, a] = Qs[t-1, m, a] + alpha * (r - Qs[t-1, m, a])\n",
    "    Qs[t, m, 1-a] = Qs[t-1, m, 1-a]\n",
    "\n",
    "# Apply the softmax transformation in a vectorized way\n",
    "obs_Qs = [Qs[i] for i in idx]\n",
    "Qs_ = np.array(obs_Qs) * beta\n",
    "#log_prob_actions = Qs_ - scipy.special.logsumexp(Qs_, axis=1)[:, None]\n",
    "\n",
    "# Return the log_prob_actions for the observed actions\n",
    "#log_prob_actions = log_prob_actions[np.arange(len(actions)), machines, actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
