{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating RL parameters using PyMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifying [this notebook](https://github.com/ricardoV94/stats/blob/master/modelling/RL_PyMC.ipynb) to simulate data and estimate parameters for the Machine Game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as collections\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation Function\n",
    "\n",
    "Initial data generation function uses an RL model with only two parameters $\\alpha$ and $\\beta$. Question for this notebook: can these parameters be recovered successfully by the different estimators (MLE vs. NUTS sampler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(alpha, beta, n=100, \n",
    "                  p_r={'high_var': [.95, .05], 'low_var': [.5,.5]},\n",
    "                  rs = np.array(([5.0, -495.0],[-5.0, 495.0],[10.0, -100.0],[-10.0, 100.0])),\n",
    "                  sQ = np.zeros((4, 2))\n",
    "                 ):\n",
    "    \n",
    "    # Need to denote both machine type and action\n",
    "    \n",
    "    # Pre-specify machines for each trial in a randomly balanced manner\n",
    "    if n%4 != 0:\n",
    "        print(\"Number of trials is not divisable by 4.\\nCreating trials for %s trials.\"%(str(n-(n%4))))\n",
    "        n = n-(n%4)\n",
    "    \n",
    "    machs = np.array([0,1,2,3])\n",
    "    machs = np.tile(machs, int(n/4))\n",
    "    np.random.shuffle(machs)\n",
    "    \n",
    "    # Initialize empty array that will be populated in the loop based on Q values\n",
    "    acts = np.zeros(n, dtype=np.int)\n",
    "    \n",
    "    # Generate by coin flip for machine with differing probabilities and outcomes\n",
    "    rews = np.zeros(n, dtype=np.int)\n",
    "\n",
    "    # Stores the expected value for each of 4 machines in each trial for each action\n",
    "    Qs = np.zeros((n, 4, 2))\n",
    "\n",
    "    # Initialize Q table\n",
    "    # Denotes expected value of each action\n",
    "    # Should look like [0, 0] for each machine\n",
    "    # *** The expected value of not playing should not change from 0! ***\n",
    "    # Could these initial expected values/beliefs also be estimated from data?\n",
    "    # E.g. what if kids have more optimistic priors about each machine though they learn at the same rate\n",
    "    Q = sQ.copy()\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        cur_machine = machs[i]\n",
    "        \n",
    "        # Apply the Softmax transformation\n",
    "        exp_Q = np.exp(np.multiply(beta, Q[cur_machine]))\n",
    "        prob_a = exp_Q / np.sum(exp_Q)\n",
    "\n",
    "        # Simulate choice\n",
    "        a = np.random.choice([0, 1], p=prob_a)\n",
    "        \n",
    "        # Simulate reward if machine is played\n",
    "        if a == 1:\n",
    "    \n",
    "            # Before sampling reward determine which variance condition machine is in\n",
    "            if cur_machine>1:\n",
    "                cur_p = 'low_var'\n",
    "            else:\n",
    "                cur_p = 'high_var'\n",
    "\n",
    "            # Sample reward for current machine given its reward probs and outcome options\n",
    "            r = np.random.choice(rs[cur_machine], p = p_r[cur_p]) \n",
    "            \n",
    "            # Update Q table only if the machine is played\n",
    "            # And only the value of playing NOT of not playing\n",
    "            Q[cur_machine][a] = Q[cur_machine][a] + alpha * (r - Q[cur_machine][a])\n",
    "        \n",
    "        # If the machine is not played then Q remains unchanged and no reward is received\n",
    "        else:\n",
    "            r = 0.0\n",
    "\n",
    "        # Store values\n",
    "        acts[i] = a\n",
    "        rews[i] = r\n",
    "        #Qs[i] = Q.copy()\n",
    "        Qs[i] = Q\n",
    "\n",
    "    return machs, acts, rews, Qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE likelihood functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llik_td(x, *args):\n",
    "    # Extract the arguments as they are passed by scipy.optimize.minimize\n",
    "    alpha, beta = x\n",
    "    machines, actions, rewards = args\n",
    "\n",
    "    # Initialize values\n",
    "    Q = np.zeros((4, 2))\n",
    "    log_prob_actions = np.zeros(len(actions))\n",
    "\n",
    "    for t, (m, a, r) in enumerate(zip(machines, actions, rewards)):\n",
    "        \n",
    "        # Apply the softmax transformation\n",
    "        Q_ = Q[m] * beta\n",
    "        #print('t: %s, m: %s, a: %s, r: %s, Q:[%s, %s]'%(str(t), str(m), str(a), str(r), str(Q[m,0]), str(Q[m, 1])))\n",
    "        log_prob_action = Q_ - scipy.special.logsumexp(Q_)\n",
    "\n",
    "        # Store the log probability of the observed action\n",
    "        log_prob_actions[t] = log_prob_action[a]\n",
    "\n",
    "        # Update the Q values for the next trial\n",
    "        # Q[a] = Q[a] + alpha * (r - Q[a])\n",
    "        Q[m][a] = Q[m][a] + alpha * (r - Q[m][a])\n",
    "\n",
    "    # Return the negative log likelihood of all observed actions\n",
    "    return -np.sum(log_prob_actions[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llik_td_vectorized(x, *args):\n",
    "    # Extract the arguments as they are passed by scipy.optimize.minimize\n",
    "    alpha, beta = x\n",
    "    machines, actions, rewards = args\n",
    "    n = len(actions)\n",
    "\n",
    "    # Create a list with the Q values of each trial\n",
    "    Qs = np.zeros((n, 4, 2), dtype=np.float)\n",
    "    \n",
    "    # The last Q values were never used, so there is no need to compute them\n",
    "    for t, (m, a, r) in enumerate(zip(machines[:-1], actions[:-1], rewards[:-1])):\n",
    "        Qs[t+1] = Qs[t]\n",
    "        Qs[t+1, m, a] = Qs[t, m, a] + alpha * (r - Qs[t, m, a])\n",
    "        Qs[t+1, m, 1-a] = Qs[t, m, 1-a]\n",
    "        #print('t: %s, m: %s, a: %s, r: %s, Q:[%s, %s]'%(str(t), str(m), str(a), str(r), str(Qs[t,m,0]), str(Qs[t,m, 1])))\n",
    "\n",
    "    # Apply the softmax transformation in a vectorized way\n",
    "    idx = list(zip(range(n),machines))\n",
    "    obs_Qs = [Qs[i] for i in idx]\n",
    "    Qs_ = np.array(obs_Qs) * beta\n",
    "    log_prob_actions = Qs_ - scipy.special.logsumexp(Qs_, axis=1)[:, None]\n",
    "\n",
    "    # Return the log_prob_actions for the observed actions\n",
    "    log_prob_obs_actions = log_prob_actions[np.arange(n), actions]\n",
    "    return -np.sum(log_prob_obs_actions[1:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_alpha = .1\n",
    "true_beta = 19\n",
    "n = 120\n",
    "machines, actions, rewards, all_Qs = generate_data(true_alpha, true_beta, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter recovery with MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = [true_alpha, true_beta]\n",
    "result = scipy.optimize.minimize(llik_td, x0, args=(machines, actions, rewards), method='BFGS')\n",
    "print(result)\n",
    "print('')\n",
    "print(f'MLE: alpha = {result.x[0]:.2f} (true value = {true_alpha})')\n",
    "print(f'MLE: beta = {result.x[1]:.2f} (true value = {true_beta})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute true likelihood of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llik_td([true_alpha, true_beta], *(machines, actions, rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llik_td_vectorized([true_alpha, true_beta], *(machines, actions, rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Theano and pyMC3 functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating likelihood of data as an example of using theano tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q(machine, action, reward,\n",
    "             Q,\n",
    "             alpha):\n",
    "    Q = tt.set_subtensor(Q[machine, action], Q[machine, action] + alpha * (reward - Q[machine, action]))\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the variables into appropriate Theano objects\n",
    "machines_ = theano.shared(np.asarray(machines, dtype='int16')) \n",
    "actions_ = theano.shared(np.asarray(actions, dtype='int16'))\n",
    "rewards_ = theano.shared(np.asarray(rewards, dtype='int16'))\n",
    "\n",
    "alpha = tt.scalar(\"alpha\")\n",
    "beta = tt.scalar(\"beta\")\n",
    "\n",
    "# Initialize the Q table\n",
    "Qs = tt.zeros((4,2), dtype='float64')\n",
    "\n",
    "# Compute the Q values for each trial\n",
    "Qs, updates = theano.scan(\n",
    "    fn=update_Q,\n",
    "    sequences=[machines_, actions_, rewards_],\n",
    "    outputs_info=[Qs],\n",
    "    non_sequences=[alpha])\n",
    "\n",
    "int_Qs = tt.zeros((1, 4,2), dtype='float64')\n",
    "\n",
    "Qs = tt.concatenate((int_Qs, Qs), axis=0)\n",
    "\n",
    "# Apply the softmax transformation\n",
    "idx = list(zip(range(n),machines)) #list of tuples\n",
    "obs_Qs = [Qs[tuple(i)] for i in idx]\n",
    "Qs_ = obs_Qs * beta\n",
    "log_prob_actions = Qs_ - pm.math.logsumexp(Qs_, axis=1)\n",
    "\n",
    "# Calculate the negative log likelihod of the observed actions\n",
    "log_prob_actions = log_prob_actions[tt.arange(actions_.shape[0]), actions_]\n",
    "neg_log_like = -tt.sum(log_prob_actions[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theano_llik_td = theano.function(inputs=[alpha, beta], outputs=[neg_log_like], updates = updates)\n",
    "result = theano_llik_td(true_alpha, true_beta)\n",
    "float(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theano likelihood calculator\n",
    "\n",
    "Wrapped up likelihood calculation in function that can be called by pyMC model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theano_llik_td(alpha, beta, machines, actions, rewards, n=120):\n",
    "    # Transform the variables into appropriate Theano objects\n",
    "    machines_ = theano.shared(np.asarray(machines, dtype='int16')) \n",
    "    actions_ = theano.shared(np.asarray(actions, dtype='int16'))\n",
    "    rewards_ = theano.shared(np.asarray(rewards, dtype='int16'))\n",
    "    \n",
    "    # Initialize the Q table\n",
    "    Qs = tt.zeros((4,2), dtype='float64')\n",
    "\n",
    "    # Compute the Q values for each trial\n",
    "    Qs, updates = theano.scan(\n",
    "        fn=update_Q,\n",
    "        sequences=[machines_, actions_, rewards_],\n",
    "        outputs_info=[Qs],\n",
    "        non_sequences=[alpha])\n",
    "\n",
    "    int_Qs = tt.zeros((1, 4,2), dtype='float64')\n",
    "\n",
    "    Qs = tt.concatenate((int_Qs, Qs), axis=0)\n",
    "\n",
    "    # Apply the softmax transformation\n",
    "    idx = list(zip(range(n),machines)) #list of tuples\n",
    "    obs_Qs = [Qs[i] for i in idx]\n",
    "    Qs_ = obs_Qs * beta\n",
    "    log_prob_actions = Qs_ - pm.math.logsumexp(Qs_, axis=1)\n",
    "\n",
    "    # Calculate the negative log likelihod of the observed actions\n",
    "    log_prob_actions = log_prob_actions[tt.arange(actions_.shape[0]), actions_]\n",
    "    return tt.sum(log_prob_actions[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative theano likelihood calculator \n",
    "\n",
    "using a right action probabilities to compare to coin flips using a Bernoilli distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def right_action_probs(alpha, beta, machines, actions, rewards):\n",
    "    # Transform the variables into appropriate Theano objects\n",
    "    machines_ = theano.shared(np.asarray(machines, dtype='int16')) \n",
    "    actions_ = theano.shared(np.asarray(actions, dtype='int16'))\n",
    "    rewards_ = theano.shared(np.asarray(rewards, dtype='int16'))\n",
    "    \n",
    "    # Initialize the Q table\n",
    "    Qs = tt.zeros((4,2), dtype='float64')\n",
    "\n",
    "    # Compute the Q values for each trial\n",
    "    Qs, updates = theano.scan(\n",
    "        fn=update_Q,\n",
    "        sequences=[machines_, actions_, rewards_],\n",
    "        outputs_info=[Qs],\n",
    "        non_sequences=[alpha])\n",
    "\n",
    "    int_Qs = tt.zeros((1, 4,2), dtype='float64')\n",
    "\n",
    "    Qs = tt.concatenate((int_Qs, Qs), axis=0)\n",
    "\n",
    "    # Apply the softmax transformation\n",
    "    idx = list(zip(range(n),machines)) #list of tuples\n",
    "    obs_Qs = [Qs[i] for i in idx]\n",
    "    Qs_ = obs_Qs * beta\n",
    "    log_prob_actions = Qs_ - pm.math.logsumexp(Qs_, axis=1)\n",
    "\n",
    "    # Calculate the negative log likelihod of the observed actions\n",
    "    \n",
    "    return tt.exp(log_prob_actions[:, 1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter recovery using pyMC3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_ = theano.shared(np.asarray(actions, dtype='int16'))\n",
    "\n",
    "with pm.Model() as m:\n",
    "    alpha = pm.Beta('alpha', 1, 1)\n",
    "    beta = pm.HalfNormal('beta', 10)\n",
    "\n",
    "    like = pm.Potential('like', theano_llik_td(alpha, beta, machines, actions, rewards))\n",
    "    \n",
    "    # The alternative gave some less stable estimates so sticking to the standard for now\n",
    "    # action_probs = right_action_probs(alpha, beta, machines, actions, rewards)\n",
    "    # like = pm.Bernoulli('like', p=action_probs, observed=actions_)\n",
    "    \n",
    "    tr = pm.sample(draws=2000, chains=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(tr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
