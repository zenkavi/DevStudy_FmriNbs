{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "# Expanding on Ito et al. (2017) to recover simulated task activity and connectivity matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import copy\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import custom modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "sys.path.append('../../utils/')\n",
    "\n",
    "# Primary module with most model functions\n",
    "import model\n",
    "\n",
    "# Module for FC regression\n",
    "import multregressionconnectivity as mreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = lambda x: np.tanh(x)\n",
    "\n",
    "inv_phi = lambda x: np.arctanh(x)\n",
    "\n",
    "def check_symmetric(a, rtol=1e-05, atol=1e-08):\n",
    "    return np.allclose(a, a.T, rtol=rtol, atol=atol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Neural network model setup\n",
    "\n",
    "In this notebook we expand on simulations described in [Ito et al. (2017)](https://www.nature.com/articles/s41467-017-01000-w.pdf). Ito et al. propose a dynamic neural network model to simulate resting state and task data. Accordingly change in activity in each node is a function of the local connectivity determined by $s$, global connectivity determined by $g$ and task activity for that node described in $I$\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{dx_i}{dt}\\tau_i = -x_i(t) + s\\phi\\big(x_i(t)\\big) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(x_j(t)\\big)\\Bigg) + I_i(t)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where transformation function $\\phi$ is\n",
    "\\begin{equation*}\n",
    "\\phi(x) = \\tanh(x) = \\frac{\\sinh(x)}{\\cosh(x)} = \\frac{e^{2x}-1}{e^{2x}+1}\n",
    "\\end{equation*}\n",
    "\n",
    "This transformation is intended to introduce *\"a nonlinearity to the interactions among units that is\n",
    "similar to aggregate nonlinearity from neuronal action potentials\"* as explained in [Cole et al. (2016)](https://www.nature.com/articles/nn.4406.pdf). The effect of this transformation can be seen below in the attenuated signal of the transformed timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"][0] = 6\n",
    "plt.rcParams[\"figure.figsize\"][1] = 5\n",
    "a = np.random.normal(size=100)\n",
    "pa = phi(a)\n",
    "plt.plot(a, label=\"Untransformed\")\n",
    "plt.plot(pa, label=\"Transformed\")\n",
    "plt.axhline(y=1,linewidth=2, color='gray', ls = \"--\")\n",
    "plt.axhline(y=-1,linewidth=2, color='gray', ls = \"--\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Ito et al. (2017) $\\tau_i$, $s$, $g$ and $dt$ are set to 1. This simplifies the equation to:\n",
    "\n",
    "\\begin{equation*}\n",
    "x_i(t) + \\frac{dx_i}{dt} = \\phi\\big(x_i(t)\\big) + \\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(x_j(t)\\big) + I_i(t)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the synaptic connectivity weight matrix:\n",
    "\n",
    "`W[..., x]` : column x of matrix denotes all outgoing connection weights from node x  \n",
    "`W[x, ...]` : row x of matrix denotes all incoming connection weights to node x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`taskdata` is an $n$ by $t$ matrix containing the timeseries for each node where $n$-rows are for nodes and $t$ columns are for the time points.\n",
    "\n",
    "It is generated by updating each column (each time point) for **all nodes**  \n",
    "\n",
    "At each time point the differential equation describing the change in activity is solved using the [Runge-Kutta second order method](https://lpsa.swarthmore.edu/NumInt/NumIntSecond.html). Runge-Kutta methods are used to discretize the problem of updating values for continuous time when solving differential equations. Ito et al. hypothesize the differential equation determining the amount of change in each timestep as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{dx_i}{dt} = \\frac{-x_i(t) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(x_j(t)\\big)\\Bigg) + s\\phi\\big(x_i(t)\\big) + I_i(t)}{\\tau_i}\n",
    "\\end{equation*}\n",
    "\n",
    "Thus `taskdata` for all nodes $x_i$ consists of the current activity plus a weighted sum of approximate changes in activity calculated using the differential equation.\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{i}(t+dt) = x_{i}(t) + \\frac{k1_{i}+k2_{i}}{2}\n",
    "\\end{equation*}\n",
    "\n",
    "The first derivative based on $t(0)$ that is used for the approximation is\n",
    "\n",
    "\\begin{equation*}\n",
    "k1_{i} = \\frac{-x_{i}(t) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t)}{\\tau}\n",
    "\\end{equation*}\n",
    "\n",
    "$\\hat{I}_{i}(t)$ is the spontaneous activity that is the sum of randomly generated noise and task-related stimulation ($\\hat{I}_{i}(t) = {I}_{i}(t)+\\epsilon(t)$). Since the column-vector of task-related stimulation activity is very sparse ($|I_{i}(t)| \\neq 0$ only for $i$ that is the third of the hub nodes and only for a short window of timepoints $t$) **most of the activity feeds into updating the activity of a node is noise (which in this framework is the equivalent of rest activity)**.\n",
    "\n",
    "Using this slope the first intermediate approximation at the endpoint is\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{i}^*(t+dt) = x_{i}(t) + k1_{i} dt\n",
    "\\end{equation*}\n",
    "\n",
    "and the second order approximation for the change in activity is\n",
    "\n",
    "\\begin{equation*}\n",
    "k2_{i} = \\frac{-x_{i}^*(t+dt) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t+1)\\big)\\Bigg) + s\\phi\\big(x_{i}^*(t+dt)\\big) + \\hat{I}_{i}(t+1)}{\\tau}\n",
    "\\end{equation*}\n",
    "\n",
    "If we discretize and work through the algebra the row-wise (node-centric) 'expanded' GLM for the column-wise (timepoint-centric) generated data using the Runge-Kutte method would be:\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{i}(t+1) = x_{i}(t) + dt\\frac{k1_{i}+k2_{i}}{2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "x_{i}(t+1) = x_{i}(t) + dt\\frac{\\frac{-x_{i}(t) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t)}{\\tau}+\\frac{-x_{i}^*(t+dt) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t+1)\\big)\\Bigg) + s\\phi\\big(x_{i}^*(t+dt)\\big) + \\hat{I}_{i}(t+1)}{\\tau}}{2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing $x_{i}^*(t+dt)$\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{i}(t+1) = x_{i}(t) + dt\\frac{\\frac{-x_{i}(t) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t)}{\\tau}+\\frac{-x_{i}(t) - k1_{i} dt + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t+1)\\big)\\Bigg) + s\\phi\\big(x_{i}(t) + k1_{i} dt\\big) + \\hat{I}_{i}(t+1)}{\\tau}}{2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write out a general equation for $x_i(t+1)$ first we replace the $k1_i$ in the equation. Since $k1_i$ appears always in the context of $x_i(t)+k1_idt$ we plug in the formula for $k1_i$ into this expression and simplify\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{i}(t) + k1_{i}dt = x_{i}(t)+dt\\frac{-x_{i}(t) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t)}{\\tau}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{i}(t) + k1_{i}dt = \\frac{\\tau-dt}{\\tau}x_{i}(t)+\\frac{dt}{\\tau}\\Bigg[g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t)\\Bigg]\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use this expression in the formula of $x_{i}(t+1)$ \n",
    "\n",
    "\\begin{equation*}\n",
    "x_{i}(t+1) = x_{i}(t) + \\frac{dt}{2\\tau}\\Bigg[-x_{i}(t) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t) - \\big(x_{i}(t) + k1_{i} dt\\big) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t+1)\\big)\\Bigg) + s\\phi\\big(x_{i}(t) + k1_{i} dt\\big) + \\hat{I}_{i}(t+1)\\Bigg]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging in $x_{i}(t) + k1_{i}dt$\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{i}(t+1) = x_{i}(t) + \\frac{dt}{2\\tau}\\Bigg[-x_{i}(t) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t) - \\big(\\frac{\\tau-dt}{\\tau}x_{i}(t)+\\frac{dt}{\\tau}\\Bigg[g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t)\\Bigg]\\big) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t+1)\\big)\\Bigg) + s\\phi\\big(\\frac{\\tau-dt}{\\tau}x_{i}(t)+\\frac{dt}{\\tau}\\Bigg[g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t)\\Bigg]\\big) + \\hat{I}_{i}(t+1)\\Bigg]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplifying common terms\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{i}(t+1) = x_{i}(t) + \\frac{dt}{2\\tau}\\Bigg[\\frac{dt-2\\tau}{\\tau}x_{i}(t) + \\frac{\\tau-dt}{\\tau}\\Bigg[g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t)\\Bigg] + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t+1)\\big)\\Bigg) + s\\phi\\big(\\frac{\\tau-dt}{\\tau}x_{i}(t)+\\frac{dt}{\\tau}\\Bigg[g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t)\\Bigg]\\big) + \\hat{I}_{i}(t+1)\\Bigg]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting $x_{i}(t)$ outside the rest of the expression\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{i}(t+1) = \\frac{2\\tau^2+dt(dt-2\\tau)}{2\\tau^2}x_{i}(t) + \\frac{dt}{2\\tau}\\Bigg[\\frac{\\tau-dt}{\\tau}\\Bigg(g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t)\\Bigg) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t+1)\\big)\\Bigg) + s\\phi\\big(\\frac{\\tau-dt}{\\tau}x_{i}(t)+\\frac{dt}{\\tau}\\Bigg(g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t)\\Bigg)\\big) + \\hat{I}_{i}(t+1)\\Bigg]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty messy expression. In words it describes the activity in the next time point for a node as a function of:\n",
    "- current node activity \n",
    "- current activity depending on connectivity with other nodes, local connectivity and task stimulation \n",
    "- activity in the next time step depending on connectivity with other nodes, first order approximation in the current node and task stimulation  \n",
    "all of which are weigted by different constants that depends on the time steps\n",
    "\n",
    "Since this is just the generalized form of the simplified extended GLM equation including all the terms that were previously omitted because their values were known to be 1 we should get the same results if we rewrite the code for the extended GLM with this equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this equation to expand the traditinal GLM. A traditional GLM would have modeled\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{i}(t) = h(t) \\circledast I(t) + \\epsilon_i(t)\n",
    "\\end{equation*}\n",
    "\n",
    "that is the activity of a voxel at a given time point would have been a function of the convolved task activity for that node at that time point plus some noise (For these simulations we ignore other movement confounds etc. included in level 1 models in real data).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Extending the GLM in this way is intended to account for the connectivity between nodes and how **task** activity in the previous step affects task activity in the current step for a given node. I don't think this is the same thing as 'subtracting out' resting state to look at the remaining task activity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended GLM functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ucr_glm(all_nodes_ts, task_reg):\n",
    "    nregions = all_nodes_ts.shape[0]\n",
    "    ucr_task_betas = np.zeros((nregions))\n",
    "    \n",
    "    for region in range(0, nregions):\n",
    "        cur_y = all_nodes_ts[region,:]\n",
    "        ucr_mod = sm.OLS(cur_y, task_reg)\n",
    "        ucr_res = ucr_mod.fit()\n",
    "        ucr_task_betas[region] = ucr_res.params[0]\n",
    "    \n",
    "    return ucr_task_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ext_glm(all_nodes_ts, task_reg, weight_matrix, dt, tau, g, s): \n",
    "    \n",
    "    nregions = all_nodes_ts.shape[0]\n",
    "    ext_task_betas = np.zeros((nregions))\n",
    "    \n",
    "    for region in range(0, nregions):\n",
    "        cur_y = all_nodes_ts[region,:]\n",
    "        incoming_connections = weight_matrix[region, :]\n",
    "        incoming_connections = np.delete(incoming_connections,region)\n",
    "        drop_region = [region]\n",
    "        \n",
    "        #DV\n",
    "        next_y = cur_y[1:] #shift column up to predict activity in next time point\n",
    "\n",
    "        #IV 1\n",
    "        cur_y = cur_y[:-1] #drop last time point\n",
    "        cur_y = ((2*(tau**2)+dt*(dt-2*tau))/(2*(tau**2)))*cur_y\n",
    "        \n",
    "        #IV 2\n",
    "        other_ns_cur_spont = np.delete(all_nodes_ts, drop_region, axis=0)[:,:-1] #dropping last col/timepoint\n",
    "        other_ns_cur_spont = other_ns_cur_spont.T\n",
    "        other_ns_cur_spont = np.apply_along_axis(phi, 0, other_ns_cur_spont)\n",
    "        other_ns_cur_spont = np.sum(other_ns_cur_spont*incoming_connections, axis = 1) \n",
    "        other_ns_cur_spont = (dt/(2*tau))*((tau-dt)/tau)*g*other_ns_cur_spont\n",
    "        \n",
    "        #IV 3\n",
    "        cur_y_phi = all_nodes_ts[region,:]\n",
    "        cur_y_phi = cur_y_phi[:-1]\n",
    "        cur_y_phi = (dt/(2*tau))*((tau-dt)/tau)*s*phi(cur_y_phi)\n",
    "        \n",
    "        #IV 4\n",
    "        cur_n_task = (dt/(2*tau))*((tau-dt)/tau)*task_reg[:-1]\n",
    "        \n",
    "        #IV 5\n",
    "        other_ns_next_spont = np.delete(all_nodes_ts, drop_region, axis=0)[:,1:] #dropping first col/timepoint\n",
    "        other_ns_next_spont = other_ns_next_spont.T\n",
    "        other_ns_next_spont = np.apply_along_axis(phi, 0, other_ns_next_spont)\n",
    "        other_ns_next_spont = np.sum(other_ns_next_spont*incoming_connections, axis = 1)\n",
    "        other_ns_next_spont = (dt/(2*tau))*g*other_ns_next_spont\n",
    "        \n",
    "        #IV 6\n",
    "        cur_n_first_appr = all_nodes_ts[region,:]\n",
    "        cur_n_first_appr = cur_n_first_appr[:-1]\n",
    "        cur_n_first_appr = ((tau-dt)/tau)*cur_n_first_appr\n",
    "        tmp = np.delete(all_nodes_ts, drop_region, axis=0)[:,:-1] #dropping last col/timepoint\n",
    "        tmp = tmp.T\n",
    "        tmp = np.apply_along_axis(phi, 0, tmp)\n",
    "        tmp = np.sum(tmp*incoming_connections, axis = 1) \n",
    "        tmp = (dt/tau)*g*tmp\n",
    "        cur_n_first_appr = cur_n_first_appr+tmp\n",
    "        tmp = all_nodes_ts[region,:]\n",
    "        tmp = tmp[:-1]\n",
    "        tmp = s*phi(tmp)\n",
    "        cur_n_first_appr = cur_n_first_appr+tmp\n",
    "        cur_n_first_appr = cur_n_first_appr+task_reg[:-1]\n",
    "        cur_n_first_appr = (dt/(2*tau))*s*phi(cur_n_first_appr)\n",
    "        \n",
    "        #IV 7\n",
    "        cur_n_next_task = (dt/(2*tau))*task_reg[1:]\n",
    "        \n",
    "        #All IVs in design matrix\n",
    "        ext_des_mat = np.concatenate((cur_y.reshape(-1,1), other_ns_cur_spont.reshape(-1,1)), 1)\n",
    "        ext_des_mat = np.concatenate((ext_des_mat, cur_y_phi.reshape(-1,1)), 1)\n",
    "        ext_des_mat = np.concatenate((ext_des_mat, cur_n_task.reshape(-1,1)), 1)\n",
    "        ext_des_mat = np.concatenate((ext_des_mat, other_ns_next_spont.reshape(-1,1)), 1)\n",
    "        ext_des_mat = np.concatenate((ext_des_mat, cur_n_first_appr.reshape(-1,1)), 1)\n",
    "        ext_des_mat = np.concatenate((ext_des_mat, cur_n_next_task.reshape(-1,1)), 1)\n",
    "\n",
    "        ext_mod = sm.OLS(next_y, ext_des_mat)\n",
    "        ext_res = ext_mod.fit()\n",
    "        ext_params = ext_res.params\n",
    "\n",
    "        ext_task_betas[region] = ext_params[6]\n",
    "    \n",
    "    return ext_task_betas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper around network simulation and regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_network_task_glm(ncommunities = 3, \n",
    "                         innetwork_dsity = .60, \n",
    "                         outnetwork_dsity = .08, \n",
    "                         hubnetwork_dsity = .25, \n",
    "                         nodespercommunity = 35, \n",
    "                         plot_network = False,\n",
    "                         dt = 1, tau = 1, g = 1, s = 1, \n",
    "                         topdown = True, bottomup = False, \n",
    "                         local_com = 1, \n",
    "                         Tmax = 100000, \n",
    "                         plot_task = False, \n",
    "                         stimsize = np.floor(35/3.0), \n",
    "                         tasktiming = None, \n",
    "                         noise = 1,\n",
    "                         stim_mag = .5,\n",
    "                         plot_glm = True):\n",
    "\n",
    "    totalnodes = nodespercommunity*ncommunities\n",
    "\n",
    "    # Construct structural matrix\n",
    "    S = model.generateStructuralNetwork(ncommunities=ncommunities,\n",
    "                                        innetwork_dsity=innetwork_dsity,\n",
    "                                        outnetwork_dsity=outnetwork_dsity,\n",
    "                                        hubnetwork_dsity=hubnetwork_dsity,\n",
    "                                        nodespercommunity=nodespercommunity,\n",
    "                                        showplot=plot_network)\n",
    "    # Construct synaptic matrix\n",
    "    W = model.generateSynapticNetwork(S, showplot=plot_network)\n",
    "\n",
    "    if plot_network:\n",
    "        plt.rcParams[\"figure.figsize\"][0] = 5\n",
    "        plt.rcParams[\"figure.figsize\"][1] = 4\n",
    "        sns.heatmap(W, xticklabels=False, yticklabels=False)\n",
    "        plt.xlabel('Regions')\n",
    "        plt.ylabel('Regions')\n",
    "        plt.title(\"Synaptic Weight Matrix -- Coupling Matrix\")\n",
    "\n",
    "    T = np.arange(0,Tmax,dt)\n",
    "\n",
    "    # Construct timing array for convolution \n",
    "    # This timing is irrespective of the task being performed\n",
    "    # Tasks are only determined by which nodes are stimulated\n",
    "    if tasktiming is None:\n",
    "        tasktiming = np.zeros((1,len(T)))\n",
    "        for t in range(len(T)):\n",
    "            if t%2000>500 and t%2000<1000:\n",
    "                tasktiming[0,t] = 1.0\n",
    "\n",
    "    if plot_task:\n",
    "        if len(T)>9999:\n",
    "            plt.plot(T[:10000], tasktiming[0,:10000])\n",
    "            plt.ylim(top = 1.2, bottom = -0.1)\n",
    "        else:\n",
    "            plt.plot(T, tasktiming[0,:])\n",
    "            plt.ylim(top = 1.2, bottom = -0.1)\n",
    "\n",
    "    stimtimes = np.zeros((totalnodes,len(T)))\n",
    "\n",
    "    # Construct a community affiliation vector\n",
    "    Ci = np.repeat(np.arange(ncommunities),nodespercommunity) \n",
    "\n",
    "    if topdown:\n",
    "        # Identify the regions associated with the hub network (hub network is by default the 0th network)\n",
    "        hub_ind = np.where(Ci==0)[0] \n",
    "        stim_nodes_td = np.arange(0, stimsize,dtype=int)\n",
    "    else:\n",
    "        stim_nodes_td = None\n",
    "    \n",
    "    if bottomup:\n",
    "        # Identify indices for one of the local communities\n",
    "        local_ind = np.where(Ci==local_com)[0] \n",
    "        # Identify efferent connections from local network to hub network\n",
    "        W_mask = np.zeros((W.shape))\n",
    "        W_mask[local_ind,hub_ind] = 1.0\n",
    "        local2hub_connects = np.multiply(W,W_mask)\n",
    "        local_regions_wcon = np.where(local2hub_connects!=0)[0]\n",
    "        local_regions_ncon = np.setdiff1d(local_ind,local_regions_wcon)\n",
    "        #Half of the stimulated local community nodes have hub connections while the other does not\n",
    "        stim_nodes_bu = np.hstack((np.random.choice(local_regions_ncon, int(np.floor(stimsize/2)), replace=False),\n",
    "                                np.random.choice(local_regions_wcon, int(stimsize-np.floor(stimsize/2)), replace=False)))\n",
    "    else:\n",
    "        stim_nodes_bu = None\n",
    "    \n",
    "    if stim_nodes_td is not None and stim_nodes_bu is not None:\n",
    "        stim_nodes = np.hstack((stim_nodes_td, stim_nodes_bu))\n",
    "    elif stim_nodes_td is not None and stim_nodes_bu is None:\n",
    "        stim_nodes = stim_nodes_td\n",
    "    else:\n",
    "        stim_nodes = stim_nodes_bu\n",
    "    \n",
    "    # When task is ON the activity for a stim_node at that time point changes the size of stim_mag\n",
    "    for t in range(len(T)):\n",
    "        if tasktiming[0,t] == 1:\n",
    "            stimtimes[stim_nodes,t] = stim_mag\n",
    "\n",
    "    #Make task data\n",
    "    out = model.networkModel(W,Tmax=Tmax,dt=dt,g=g,s=s,tau=tau, I=stimtimes, noise=noise)\n",
    "    taskdata = out[0]\n",
    "    \n",
    "    #Use only a subset of data for GLM's if it's too long\n",
    "    if taskdata.shape[1]>44999:\n",
    "        short_lim = int(np.floor(taskdata.shape[1]/3))\n",
    "        y = copy.copy(taskdata[:,:short_lim])\n",
    "        I = copy.copy(stimtimes[:,:short_lim])\n",
    "    else:\n",
    "        y = copy.copy(taskdata)\n",
    "        I = copy.copy(stimtimes)\n",
    "\n",
    "    # Run uncorrected and extended GLM to compare task regressor\n",
    "    ucr_glm = run_ucr_glm(all_nodes_ts = y, task_reg = I[stim_nodes[0],:])\n",
    "    ext_glm = run_ext_glm(all_nodes_ts = y, task_reg = I[stim_nodes[0],:], \n",
    "                          weight_matrix = W, dt = dt, tau = tau, g = g, s = s)\n",
    "        \n",
    "    return(W, ucr_glm, ext_glm, stim_nodes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_W, base_ucr_glm, base_ext_glm, base_stim_nodes = sim_network_task_glm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncoms = 3\n",
    "nnods = 35\n",
    "true_task_stim = np.zeros(ncoms*nnods)\n",
    "true_task_stim[base_stim_nodes] = .5\n",
    "plt.rcParams[\"figure.figsize\"][0] = 8\n",
    "plt.rcParams[\"figure.figsize\"][1] = 6\n",
    "plt.plot(b, color = \"blue\", label = \"Classic GLM\")\n",
    "plt.plot(c, color = \"green\", label = \"Extended GLM\")\n",
    "plt.plot(true_task_stim, color = \"black\", label = \"True task activity\")\n",
    "plt.ylabel('Beta',fontsize=14)\n",
    "plt.xlabel('Node',fontsize=14)\n",
    "for n in range(1,ncoms):\n",
    "    plt.axvline(x=nnods*n,linewidth=2, color='gray', ls = \"--\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing different network structures\n",
    "\n",
    "Above we tested if true task activity can be recovered when accounting for connectivity in a neural network with one hub and two local networks.\n",
    "\n",
    "Various parameters control the structure of this network and they can be maniupulated \n",
    "\n",
    "Thing to modulate:   \n",
    "- hub and local network density  \n",
    "- local ($s$) and global ($g$) information transfer strength (change in conjunction)  \n",
    "- time step ($\\tau$)\n",
    "- number local networks   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing network density\n",
    "\n",
    "#### Increased innetwork density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_W, new_ucr_glm, new_ext_glm, new_stim_nodes = sim_network_task_glm(innetwork_dsity = .85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increased outnetwork density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increased hub network density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing information transfer strength\n",
    "\n",
    "#### Increasing $g$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increasing $s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing $\\tau$\n",
    "\n",
    "#### Increasing $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decreasing $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing number of local communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing temporal resolution\n",
    "\n",
    "#### Decreasing $dt$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increasing $dt$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing different task structures\n",
    "\n",
    "The initial task was a top down task that stimulated nodes only in the hub network and trickled activity down to the other nodes.\n",
    "\n",
    "Other task activations are possible.\n",
    "\n",
    "Things to modulate:  \n",
    "- Number of nodes stimulated\n",
    "- Stimulating only local community\n",
    "- Stimulating both hub and local community\n",
    "- Magnitude of stimulation (different from 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing number of stimulated nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stimulating only local community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stimulating both hub and local community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing magnitude of stimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Accounting for whether and how the node is connected to other nodes improves the task estimates both stimulated and non-stimulated nodes\n",
    "    - Is the over- and underestimation real? If so, what is causing it?\n",
    "2. The improvement in task estimates when accounting for connectivity is NOT affected by:\n",
    "    - Network density\n",
    "    - Number of local communities\n",
    "3. The improvement in task estimates when accounting for connectivity IS affected by:\n",
    "    - Information transfer strength (both local and global)  \n",
    "    - $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
