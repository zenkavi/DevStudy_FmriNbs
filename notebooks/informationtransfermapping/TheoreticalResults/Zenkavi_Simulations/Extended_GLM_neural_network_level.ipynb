{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "# Expanding on Ito et al. (2017) to recover simulated task activity using connectivity matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import copy\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import custom modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "sys.path.append('../../utils/')\n",
    "\n",
    "# Primary module with most model functions\n",
    "import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = lambda x: np.tanh(x)\n",
    "\n",
    "inv_phi = lambda x: np.arctanh(x)\n",
    "\n",
    "def check_symmetric(a, rtol=1e-05, atol=1e-08):\n",
    "    return np.allclose(a, a.T, rtol=rtol, atol=atol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Neural network model\n",
    "\n",
    "In this notebook we expand on simulations described in [Ito et al. (2017)](https://www.nature.com/articles/s41467-017-01000-w.pdf). Ito et al. propose a dynamic neural network model to simulate resting state and task data. Accordingly change in activity in each node is a function of the local connectivity determined by $s$, global connectivity determined by $g$ and task activity for that node described in $I$\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{dx_i}{dt}\\tau_i = -x_i(t) + s\\phi\\big(x_i(t)\\big) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(x_j(t)\\big)\\Bigg) + I_i(t)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where transformation function $\\phi$ is\n",
    "\\begin{equation*}\n",
    "\\phi(x) = \\tanh(x) = \\frac{\\sinh(x)}{\\cosh(x)} = \\frac{e^{2x}-1}{e^{2x}+1}\n",
    "\\end{equation*}\n",
    "\n",
    "This transformation is intended to introduce *\"a nonlinearity to the interactions among units that is\n",
    "similar to aggregate nonlinearity from neuronal action potentials\"* as explained in [Cole et al. (2016)](https://www.nature.com/articles/nn.4406.pdf). The effect of this transformation can be seen below in the attenuated signal of the transformed timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"][0] = 6\n",
    "plt.rcParams[\"figure.figsize\"][1] = 5\n",
    "a = np.random.normal(size=100)\n",
    "pa = phi(a)\n",
    "plt.plot(a, label=\"Untransformed\")\n",
    "plt.plot(pa, label=\"Transformed\")\n",
    "plt.axhline(y=1,linewidth=2, color='gray', ls = \"--\")\n",
    "plt.axhline(y=-1,linewidth=2, color='gray', ls = \"--\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Ito et al. (2017) $\\tau_i$, $s$, $g$ and $dt$ are set to 1. This simplifies the equation to:\n",
    "\n",
    "\\begin{equation*}\n",
    "x_i(t) + \\frac{dx_i}{dt} = \\phi\\big(x_i(t)\\big) + \\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(x_j(t)\\big) + I_i(t)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the synaptic connectivity weight matrix:\n",
    "\n",
    "`W[..., x]` : column x of matrix denotes all outgoing connection weights from node x  \n",
    "`W[x, ...]` : row x of matrix denotes all incoming connection weights to node x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`taskdata` is an $n$ by $t$ matrix containing the timeseries for each node where $n$-rows are for nodes and $t$ columns are for the time points.\n",
    "\n",
    "It is generated by updating each column (each time point) for **all nodes**  \n",
    "\n",
    "At each time point the differential equation describing the change in activity is solved using the [Runge-Kutta second order method](https://lpsa.swarthmore.edu/NumInt/NumIntSecond.html). Runge-Kutta methods are used to discretize the problem of updating values for continuous time when solving differential equations. Ito et al. hypothesize the differential equation determining the amount of change in each timestep as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{dx_i}{dt} = \\frac{-x_i(t) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(x_j(t)\\big)\\Bigg) + s\\phi\\big(x_i(t)\\big) + I_i(t)}{\\tau_i}\n",
    "\\end{equation*}\n",
    "\n",
    "Thus `taskdata` for all nodes $x_i$ consists of the current activity plus a weighted sum of approximate changes in activity calculated using the differential equation.\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{i}(t+dt) = x_{i}(t) + \\frac{k1_{i}+k2_{i}}{2}\n",
    "\\end{equation*}\n",
    "\n",
    "The first derivative based on $t(0)$ that is used for the approximation is\n",
    "\n",
    "\\begin{equation*}\n",
    "k1_{i} = \\frac{-x_{i}(t) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t)}{\\tau}\n",
    "\\end{equation*}\n",
    "\n",
    "$\\hat{I}_{i}(t)$ is the spontaneous activity that is the sum of randomly generated noise and task-related stimulation ($\\hat{I}_{i}(t) = {I}_{i}(t)+\\epsilon(t)$). Since the column-vector of task-related stimulation activity is very sparse ($|I_{i}(t)| \\neq 0$ only for $i$ that is the third of the hub nodes and only for a short window of timepoints $t$) **most of the activity feeds into updating the activity of a node is noise (which in this framework is the equivalent of rest activity)**.\n",
    "\n",
    "Using this slope the first intermediate approximation at the endpoint is\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{i}^*(t+dt) = x_{i}(t) + k1_{i} dt\n",
    "\\end{equation*}\n",
    "\n",
    "and the second order approximation for the change in activity is\n",
    "\n",
    "\\begin{equation*}\n",
    "k2_{i} = \\frac{-x_{i}^*(t+dt) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t+1)\\big)\\Bigg) + s\\phi\\big(x_{i}^*(t+dt)\\big) + \\hat{I}_{i}(t+1)}{\\tau}\n",
    "\\end{equation*}\n",
    "\n",
    "If we discretize and work through the algebra the row-wise (node-centric) 'expanded' GLM for the column-wise (timepoint-centric) generated data using the Runge-Kutte method would be:\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{i}(t+1) = x_{i}(t) + dt\\frac{k1_{i}+k2_{i}}{2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "x_{i}(t+1) = x_{i}(t) + dt\\frac{\\frac{-x_{i}(t) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t)}{\\tau}+\\frac{-x_{i}^*(t+dt) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t+1)\\big)\\Bigg) + s\\phi\\big(x_{i}^*(t+dt)\\big) + \\hat{I}_{i}(t+1)}{\\tau}}{2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing $x_{i}^*(t+dt)$\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{i}(t+1) = x_{i}(t) + dt\\frac{\\frac{-x_{i}(t) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t)}{\\tau}+\\frac{-x_{i}(t) - k1_{i} dt + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t+1)\\big)\\Bigg) + s\\phi\\big(x_{i}(t) + k1_{i} dt\\big) + \\hat{I}_{i}(t+1)}{\\tau}}{2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write out a general equation for $x_i(t+1)$ first we replace the $k1_i$ in the equation. Since $k1_i$ appears always in the context of $x_i(t)+k1_idt$ we plug in the formula for $k1_i$ into this expression and simplify\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{i}(t) + k1_{i}dt = x_{i}(t)+dt\\frac{-x_{i}(t) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t)}{\\tau}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{i}(t) + k1_{i}dt = \\frac{\\tau-dt}{\\tau}x_{i}(t)+\\frac{dt}{\\tau}\\Bigg[g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t)\\Bigg]\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use this expression in the formula of $x_{i}(t+1)$ \n",
    "\n",
    "\\begin{equation*}\n",
    "x_{i}(t+1) = x_{i}(t) + \\frac{dt}{2\\tau}\\Bigg[-x_{i}(t) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t) - \\big(x_{i}(t) + k1_{i} dt\\big) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t+1)\\big)\\Bigg) + s\\phi\\big(x_{i}(t) + k1_{i} dt\\big) + \\hat{I}_{i}(t+1)\\Bigg]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging in $x_{i}(t) + k1_{i}dt$\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{i}(t+1) = x_{i}(t) + \\frac{dt}{2\\tau}\\Bigg[-x_{i}(t) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t) - \\big(\\frac{\\tau-dt}{\\tau}x_{i}(t)+\\frac{dt}{\\tau}\\Bigg[g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t)\\Bigg]\\big) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t+1)\\big)\\Bigg) + s\\phi\\big(\\frac{\\tau-dt}{\\tau}x_{i}(t)+\\frac{dt}{\\tau}\\Bigg[g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t)\\Bigg]\\big) + \\hat{I}_{i}(t+1)\\Bigg]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplifying common terms\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{i}(t+1) = x_{i}(t) + \\frac{dt}{2\\tau}\\Bigg[\\frac{dt-2\\tau}{\\tau}x_{i}(t) + \\frac{\\tau-dt}{\\tau}\\Bigg[g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t)\\Bigg] + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t+1)\\big)\\Bigg) + s\\phi\\big(\\frac{\\tau-dt}{\\tau}x_{i}(t)+\\frac{dt}{\\tau}\\Bigg[g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t)\\Bigg]\\big) + \\hat{I}_{i}(t+1)\\Bigg]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting $x_{i}(t)$ outside the rest of the expression\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{i}(t+1) = \\frac{2\\tau^2+dt(dt-2\\tau)}{2\\tau^2}x_{i}(t) + \\frac{dt}{2\\tau}\\Bigg[\\frac{\\tau-dt}{\\tau}\\Bigg(g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t)\\Bigg) + g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t+1)\\big)\\Bigg) + s\\phi\\big(\\frac{\\tau-dt}{\\tau}x_{i}(t)+\\frac{dt}{\\tau}\\Bigg(g\\Bigg(\\sum_{j\\neq i}^{N} W_{ij}\\phi\\big(\\hat{I}_j(t)\\big)\\Bigg) + s\\phi\\big(x_{i}(t)\\big) + \\hat{I}_{i}(t)\\Bigg)\\big) + \\hat{I}_{i}(t+1)\\Bigg]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty messy expression. In words it describes the activity in the next time point for a node as a function of:\n",
    "- current node activity \n",
    "- current activity depending on connectivity with other nodes, local connectivity and task stimulation \n",
    "- activity in the next time step depending on connectivity with other nodes, first order approximation in the current node and task stimulation  \n",
    "all of which are weigted by different constants that depends on the time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this equation to expand the traditinal GLM. A traditional GLM would have modeled\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{i}(t) = h(t) \\circledast I(t)\n",
    "\\end{equation*}\n",
    "\n",
    "that is the activity of a voxel at a given time point would have been a function of the convolved task activity for that node at that time point plus some noise (For these simulations we ignore other movement confounds etc. included in level 1 models in real data).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Extending the GLM in this way is intended to account for the connectivity between nodes and how **task** activity in the previous step affects task activity in the current step for a given node. Is the same thing as 'subtracting out' resting state to look at the remaining task activity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cGLM function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ucr_glm(all_nodes_ts, task_reg):\n",
    "    nregions = all_nodes_ts.shape[0]\n",
    "    ucr_task_betas = np.zeros((nregions))\n",
    "    ucr_mods = []\n",
    "    \n",
    "    for region in range(0, nregions):\n",
    "        cur_y = all_nodes_ts[region,:]\n",
    "        ucr_mod = sm.OLS(cur_y, task_reg)\n",
    "        ucr_res = ucr_mod.fit()\n",
    "        ucr_task_betas[region] = ucr_res.params[0]\n",
    "        ucr_mods.append(ucr_mod)\n",
    "    \n",
    "    return ({\"ucr_task_betas\":ucr_task_betas,\n",
    "            \"ucr_mods\": ucr_mods})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eGLM function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ext_glm(all_nodes_ts, task_reg, weight_matrix, dt, tau, g, s): \n",
    "    \n",
    "    nregions = all_nodes_ts.shape[0]\n",
    "    ext_task_betas = np.zeros((nregions))\n",
    "    ext_mods = []\n",
    "    \n",
    "    for region in range(0, nregions):\n",
    "        cur_y = all_nodes_ts[region,:]\n",
    "        incoming_connections = weight_matrix[region, :]\n",
    "        incoming_connections = np.delete(incoming_connections,region)\n",
    "        drop_region = [region]\n",
    "        \n",
    "        #DV\n",
    "        next_y = cur_y[1:] #shift column up to predict activity in next time point\n",
    "\n",
    "        #IV 1\n",
    "        cur_y = cur_y[:-1] #drop last time point\n",
    "        cur_y = ((2*(tau**2)+dt*(dt-2*tau))/(2*(tau**2)))*cur_y\n",
    "        \n",
    "        #IV 2\n",
    "        other_ns_cur_spont = np.delete(all_nodes_ts, drop_region, axis=0)[:,:-1] #dropping last col/timepoint\n",
    "        other_ns_cur_spont = other_ns_cur_spont.T\n",
    "        other_ns_cur_spont = np.apply_along_axis(phi, 0, other_ns_cur_spont)\n",
    "        other_ns_cur_spont = np.sum(other_ns_cur_spont*incoming_connections, axis = 1) \n",
    "        other_ns_cur_spont = (dt/(2*tau))*((tau-dt)/tau)*g*other_ns_cur_spont\n",
    "        \n",
    "        #IV 3\n",
    "        cur_y_phi = all_nodes_ts[region,:]\n",
    "        cur_y_phi = cur_y_phi[:-1]\n",
    "        cur_y_phi = (dt/(2*tau))*((tau-dt)/tau)*s*phi(cur_y_phi)\n",
    "        \n",
    "        #IV 4\n",
    "        cur_n_task = (dt/(2*tau))*((tau-dt)/tau)*task_reg[:-1]\n",
    "        \n",
    "        #IV 5\n",
    "        other_ns_next_spont = np.delete(all_nodes_ts, drop_region, axis=0)[:,1:] #dropping first col/timepoint\n",
    "        other_ns_next_spont = other_ns_next_spont.T\n",
    "        other_ns_next_spont = np.apply_along_axis(phi, 0, other_ns_next_spont)\n",
    "        other_ns_next_spont = np.sum(other_ns_next_spont*incoming_connections, axis = 1)\n",
    "        other_ns_next_spont = (dt/(2*tau))*g*other_ns_next_spont\n",
    "        \n",
    "        #IV 6\n",
    "        cur_n_first_appr = all_nodes_ts[region,:]\n",
    "        cur_n_first_appr = cur_n_first_appr[:-1]\n",
    "        cur_n_first_appr = ((tau-dt)/tau)*cur_n_first_appr\n",
    "        tmp = np.delete(all_nodes_ts, drop_region, axis=0)[:,:-1] #dropping last col/timepoint\n",
    "        tmp = tmp.T\n",
    "        tmp = np.apply_along_axis(phi, 0, tmp)\n",
    "        tmp = np.sum(tmp*incoming_connections, axis = 1) \n",
    "        tmp = (dt/tau)*g*tmp\n",
    "        cur_n_first_appr = cur_n_first_appr+tmp\n",
    "        tmp = all_nodes_ts[region,:]\n",
    "        tmp = tmp[:-1]\n",
    "        tmp = s*phi(tmp)\n",
    "        cur_n_first_appr = cur_n_first_appr+tmp\n",
    "        cur_n_first_appr = cur_n_first_appr+task_reg[:-1]\n",
    "        cur_n_first_appr = (dt/(2*tau))*s*phi(cur_n_first_appr)\n",
    "        \n",
    "        #IV 7\n",
    "        cur_n_next_task = (dt/(2*tau))*task_reg[1:]\n",
    "        \n",
    "        #All IVs in design matrix\n",
    "        ext_des_mat = np.concatenate((cur_y.reshape(-1,1), other_ns_cur_spont.reshape(-1,1)), 1)\n",
    "        ext_des_mat = np.concatenate((ext_des_mat, cur_y_phi.reshape(-1,1)), 1)\n",
    "        ext_des_mat = np.concatenate((ext_des_mat, cur_n_task.reshape(-1,1)), 1)\n",
    "        ext_des_mat = np.concatenate((ext_des_mat, other_ns_next_spont.reshape(-1,1)), 1)\n",
    "        ext_des_mat = np.concatenate((ext_des_mat, cur_n_first_appr.reshape(-1,1)), 1)\n",
    "        ext_des_mat = np.concatenate((ext_des_mat, cur_n_next_task.reshape(-1,1)), 1)\n",
    "\n",
    "        ext_mod = sm.OLS(next_y, ext_des_mat)\n",
    "        ext_res = ext_mod.fit()\n",
    "        ext_params = ext_res.params\n",
    "\n",
    "        ext_task_betas[region] = ext_params[6]\n",
    "        ext_mods.append(ext_mod)\n",
    "    \n",
    "    return ({\"ext_task_betas\": ext_task_betas,\n",
    "            \"ext_mods\": ext_mods})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stimulus timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stimtimes(Tmax, dt, stim_nodes, stim_mag, tasktiming=None, ncommunities = 3,nodespercommunity = 35):\n",
    "    totalnodes = nodespercommunity*ncommunities\n",
    "    T = np.arange(0,Tmax,dt)\n",
    "    # Construct timing array for convolution \n",
    "    # This timing is irrespective of the task being performed\n",
    "    # Tasks are only determined by which nodes are stimulated\n",
    "    if tasktiming is None:\n",
    "        tasktiming = np.zeros((1,len(T)))\n",
    "        for t in range(len(T)):\n",
    "            if t%2000>500 and t%2000<1000:\n",
    "                tasktiming[0,t] = 1.0\n",
    "    stimtimes = np.zeros((totalnodes,len(T)))\n",
    "    \n",
    "    # When task is ON the activity for a stim_node at that time point changes the size of stim_mag\n",
    "    for t in range(len(T)):\n",
    "        if tasktiming[0,t] == 1:\n",
    "            stimtimes[stim_nodes,t] = stim_mag\n",
    "            \n",
    "    return(stimtimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_network_task_glm(ncommunities = 3, \n",
    "                         innetwork_dsity = .60, \n",
    "                         outnetwork_dsity = .08, \n",
    "                         hubnetwork_dsity = .25, \n",
    "                         nodespercommunity = 35, \n",
    "                         plot_network = False,\n",
    "                         dt = 1, tau = 1, g = 1, s = 1, \n",
    "                         topdown = True, bottomup = False, \n",
    "                         local_com = 1, \n",
    "                         Tmax = 100000, \n",
    "                         plot_task = False, \n",
    "                         stimsize = np.floor(35/3.0), \n",
    "                         tasktiming = None, \n",
    "                         noise = None,\n",
    "                         noise_loc = 0, \n",
    "                         noise_scale = 0,\n",
    "                         stim_mag = .5,\n",
    "                         plot_glm = True):\n",
    "\n",
    "    totalnodes = nodespercommunity*ncommunities\n",
    "\n",
    "    # Construct structural matrix\n",
    "    S = model.generateStructuralNetwork(ncommunities=ncommunities,\n",
    "                                        innetwork_dsity=innetwork_dsity,\n",
    "                                        outnetwork_dsity=outnetwork_dsity,\n",
    "                                        hubnetwork_dsity=hubnetwork_dsity,\n",
    "                                        nodespercommunity=nodespercommunity,\n",
    "                                        showplot=plot_network)\n",
    "    # Construct synaptic matrix\n",
    "    W = model.generateSynapticNetwork(S, showplot=plot_network)\n",
    "\n",
    "    if plot_network:\n",
    "        plt.rcParams[\"figure.figsize\"][0] = 5\n",
    "        plt.rcParams[\"figure.figsize\"][1] = 4\n",
    "        sns.heatmap(W, xticklabels=False, yticklabels=False)\n",
    "        plt.xlabel('Regions')\n",
    "        plt.ylabel('Regions')\n",
    "        plt.title(\"Synaptic Weight Matrix -- Coupling Matrix\")\n",
    "\n",
    "    T = np.arange(0,Tmax,dt)\n",
    "\n",
    "    # Construct timing array for convolution \n",
    "    # This timing is irrespective of the task being performed\n",
    "    # Tasks are only determined by which nodes are stimulated\n",
    "    if tasktiming is None:\n",
    "        tasktiming = np.zeros((1,len(T)))\n",
    "        for t in range(len(T)):\n",
    "            if t%2000>500 and t%2000<1000:\n",
    "                tasktiming[0,t] = 1.0\n",
    "\n",
    "    if plot_task:\n",
    "        if len(T)>9999:\n",
    "            plt.plot(T[:10000], tasktiming[0,:10000])\n",
    "            plt.ylim(top = 1.2, bottom = -0.1)\n",
    "        else:\n",
    "            plt.plot(T, tasktiming[0,:])\n",
    "            plt.ylim(top = 1.2, bottom = -0.1)\n",
    "\n",
    "    stimtimes = np.zeros((totalnodes,len(T)))\n",
    "\n",
    "    # Construct a community affiliation vector\n",
    "    Ci = np.repeat(np.arange(ncommunities),nodespercommunity) \n",
    "    # Identify the regions associated with the hub network (hub network is by default the 0th network)\n",
    "    hub_ind = np.where(Ci==0)[0] \n",
    "\n",
    "    if topdown:\n",
    "        stim_nodes_td = np.arange(0, stimsize,dtype=int)\n",
    "    else:\n",
    "        stim_nodes_td = None\n",
    "    \n",
    "    if bottomup:\n",
    "        # Identify indices for one of the local communities\n",
    "        local_ind = np.where(Ci==local_com)[0] \n",
    "        # Identify efferent connections from local network to hub network\n",
    "        W_mask = np.zeros((W.shape))\n",
    "        W_mask[local_ind,hub_ind] = 1.0\n",
    "        local2hub_connects = np.multiply(W,W_mask)\n",
    "        local_regions_wcon = np.where(local2hub_connects!=0)[0]\n",
    "        local_regions_ncon = np.setdiff1d(local_ind,local_regions_wcon)\n",
    "        #If there are enough nodes in the local community with hub connections:\n",
    "        if len(local_regions_wcon)>= np.floor(stimsize/2):\n",
    "            #Half of the stimulated local community nodes have hub connections while the other does not\n",
    "            stim_nodes_bu = np.hstack((np.random.choice(local_regions_ncon, int(np.floor(stimsize/2)), replace=False),\n",
    "                                np.random.choice(local_regions_wcon, int(stimsize-np.floor(stimsize/2)), replace=False)))\n",
    "        else:\n",
    "            stim_nodes_bu = np.hstack((np.random.choice(local_regions_wcon, len(local_regions_wcon), replace=False),\n",
    "                                np.random.choice(local_regions_ncon, int(stimsize-len(local_regions_wcon)), replace=False)))\n",
    "    else:\n",
    "        stim_nodes_bu = None\n",
    "    \n",
    "    if stim_nodes_td is not None and stim_nodes_bu is not None:\n",
    "        stim_nodes = np.hstack((stim_nodes_td, stim_nodes_bu))\n",
    "    elif stim_nodes_td is not None and stim_nodes_bu is None:\n",
    "        stim_nodes = stim_nodes_td\n",
    "    else:\n",
    "        stim_nodes = stim_nodes_bu\n",
    "    \n",
    "    # When task is ON the activity for a stim_node at that time point changes the size of stim_mag\n",
    "    for t in range(len(T)):\n",
    "        if tasktiming[0,t] == 1:\n",
    "            stimtimes[stim_nodes,t] = stim_mag\n",
    "\n",
    "    #Make task data\n",
    "    out = model.networkModel(W,Tmax=Tmax,dt=dt,g=g,s=s,tau=tau, I=stimtimes, noise=noise, noise_loc = noise_loc, noise_scale = noise_scale)\n",
    "    taskdata = out[0]\n",
    "    \n",
    "    #Use only a subset of data for GLM's if it's too long\n",
    "    if taskdata.shape[1]>44999:\n",
    "        short_lim = int(np.floor(taskdata.shape[1]/3))\n",
    "        y = copy.copy(taskdata[:,:short_lim])\n",
    "        I = copy.copy(stimtimes[:,:short_lim])\n",
    "    else:\n",
    "        y = copy.copy(taskdata)\n",
    "        I = copy.copy(stimtimes)\n",
    "\n",
    "    # Run uncorrected and extended GLM to compare task regressor\n",
    "    ucr_model = run_ucr_glm(all_nodes_ts = y, task_reg = I[stim_nodes[0],:])\n",
    "    ext_model = run_ext_glm(all_nodes_ts = y, task_reg = I[stim_nodes[0],:], \n",
    "                          weight_matrix = W, dt = dt, tau = tau, g = g, s = s)\n",
    "    \n",
    "    ucr_betas = ucr_model[\"ucr_task_betas\"]\n",
    "    ext_betas = ext_model[\"ext_task_betas\"]\n",
    "    \n",
    "    ucr_glms = ucr_model[\"ucr_mods\"]\n",
    "    ext_glms = ext_model[\"ext_mods\"]\n",
    "        \n",
    "    return({\"W\":W, \"ucr_betas\": ucr_betas, \"ucr_glms\": ucr_glms, \"ext_betas\": ext_betas, \"ext_glms\": ext_glms,\n",
    "            \"stim_nodes\": stim_nodes, \"taskdata\": taskdata})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline calculation\n",
    "\n",
    "Need different ones for cGLM and eGLM\n",
    "Or you want just the eGLM baseline\n",
    "which would be the relationship between the residualiazed timeseries (after taking into account the amplification through the rest of the network) and the actual task\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_res_ts(sim):\n",
    "    \n",
    "    num_nodes = sim['taskdata'].shape[0]\n",
    "    num_ts = len(sim['ext_glms'][0].endog)\n",
    "    \n",
    "    res_ts = np.zeros((num_nodes, num_ts))\n",
    "    \n",
    "    for cur_node in range(res_ts.shape[0]):\n",
    "        raw_y = sim['ext_glms'][cur_node].endog\n",
    "        res_x = sim['ext_glms'][cur_node].exog[:,:-1]\n",
    "        res_mod = sm.OLS(raw_y, res_x).fit()\n",
    "        res_ts[cur_node,:] = res_mod.resid\n",
    "        \n",
    "    return(res_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_baseline(sim, stim_nodes = np.array(range(11)), nonstim_nodes = np.array(range(11, 105)), stim_mag = 0.5):\n",
    "    \n",
    "    res_ts = get_res_ts(sim)\n",
    "    \n",
    "    stim_t = []\n",
    "    nonstim_t = []\n",
    "    \n",
    "    for t in range(len(res_ts[0])):\n",
    "        if t%2000 == 500:\n",
    "        #Don't take out exact task timing but the period when the node has settled into the max activity \n",
    "        #if t%2000>500 and t%2000<1000:\n",
    "            stim_t.append(res_ts[stim_nodes,t])\n",
    "        elif t%2000 == 900:\n",
    "            nonstim_t.append(res_ts[nonstim_nodes, t])\n",
    "            \n",
    "    stim_t = flatten(stim_t)\n",
    "    nonstim_t = flatten(nonstim_t)\n",
    "     \n",
    "    stim_baseline = np.mean(stim_t)/stim_mag\n",
    "    nonstim_baseline = np.mean(nonstim_t)/stim_mag\n",
    "    \n",
    "    return(stim_baseline, nonstim_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sim_network_glm(data,\n",
    "                         width = 8,\n",
    "                        height = 6,\n",
    "                        ncoms = 3,\n",
    "                        nnods = 35,\n",
    "                        task_type = \"td\",\n",
    "                        ucr_label = \"cGLM (baseline)\",\n",
    "                        ext_label = \"eGLM (baseline)\",\n",
    "                         base_label = None,\n",
    "                        alp = 1):\n",
    "    \n",
    "    totalnodes = ncoms*nnods\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"][0] = width\n",
    "    plt.rcParams[\"figure.figsize\"][1] = height\n",
    "    \n",
    "    plt.plot(data['ucr_betas'], alpha = alp, color = \"C0\", label = ucr_label)\n",
    "    plt.plot(data['ext_betas'], alpha = alp, color = \"C1\", label = ext_label)\n",
    "    \n",
    "    stim_baseline, nonstim_baseline = get_true_baseline(data)\n",
    "    \n",
    "\n",
    "    all_nodes = list(range(totalnodes))\n",
    "    stim_ind = [1 if x in data['stim_nodes'] else 0 for x in all_nodes]\n",
    "    baseline_vec = [stim_baseline if x == 1 else nonstim_baseline for x in stim_ind]\n",
    "    plt.plot(baseline_vec, \n",
    "     color = \"black\", linestyle = '--', label = base_label, alpha = alp)\n",
    "    \n",
    "    plt.ylabel('Beta',fontsize=14)\n",
    "    plt.xlabel('Node',fontsize=14)\n",
    "    \n",
    "    for n in range(1,ncoms):\n",
    "        plt.axvline(x=nnods*n,linewidth=2, color='gray', ls = \"--\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline simulation\n",
    "\n",
    "First we simulate a topdown task that stimulates nodes only in the hub network. No noise is added to this simulation. It is intended to serve as a baseline to compare the changes for all the tests below. \n",
    "\n",
    "The x-axis denoted each node and the y-axis the regression coefficient between the task activity and each nodes' time series. The blue line the parameters using the classic GLM framework (cGLM) and the yellow line are the parameter estimates when the time series is corrected for its connectivity with the rest of the network using the extended GLM equation above (eGLM). The vertical lines denote community boundaries. In the baseline simulation there is one hub network and two local networks. The horizontal line depicts the expected magnitude of the task parameter accounting for that nodes connectivity and stimulation status (detailed below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_sim = sim_network_task_glm(noise=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sim_network_glm(data = base_sim, base_label = \"True baseline\")\n",
    "plt.text(x = 15, y = 3.5, s = \"Hub\", fontsize = 14)\n",
    "plt.text(x = 45, y = 3.5, s = \"Local 1\", fontsize = 14)\n",
    "plt.text(x = 85, y = 3.5, s = \"Local 2\", fontsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the change in the GLM equation the task parameter estimates can get change in two ways:\n",
    "\n",
    "- bias [higher mean]: the correlation between the timeseries and task regressor is stronger for a given node\n",
    "- variance [more variable estimates]: the correlation between the timeseries and the task regressor varies more across nodes. Since the task regressor is the same for all nodes (since we wouldn't empirically know which nodes are stimulated by it) this increased variability would have to be due to increased variability in the timeseries across nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different noise levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_stimtimes_0_5 = make_stimtimes(Tmax=10000, dt=1, stim_nodes=base_sim['stim_nodes'], stim_mag=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_ts = {}\n",
    "\n",
    "noise_sds = [0, .5, 1, 2, 5]\n",
    "\n",
    "for noise_sd in noise_sds:\n",
    "    var_name = \"noise_\"+str(noise_sd)\n",
    "    noise_ts[var_name] = model.networkModel(base_sim['W'],Tmax=10000,dt=1,g=1,s=1,tau=1, I=td_stimtimes_0_5, noise=1, noise_scale = noise_sd)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we examine the effect of different amounts of noise. When the noise parameter is turned on a random number from a normal distribution with a mean of 0 and specified standard deviation is added to the activity level at each time point. The amount of noise is changed by modifying the variance of this distribution and leaving the mean at 0. Changing the amount of noise added to each time point we look at  \n",
    "- the timeseries of a stimulated and non-stimulated node  \n",
    "- the effect of eGLM parameter estimates  \n",
    "- the relationship between task timing and activity levels  \n",
    "- the effect of eGLM on time series of nonstimulated nodes  \n",
    "\n",
    "The time series for data generated without noise points out the amplification of the task signal through connections to the rest of the network. This is why the activity for a non-stimulated node (right plot) is not at 0 for the whole task either. Importantly, this suggests that the \"correct\" baseline that the task activity parameter estimates should be compared to is not the stimulus timing for each node but the amplified activity based on the rest of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"][0] = 18\n",
    "plt.rcParams[\"figure.figsize\"][1] = 6\n",
    "\n",
    "#Plot same stimulated node's timeseries with and without noise\n",
    "fig,a =  plt.subplots(1,2)\n",
    "curns = [0, 21]\n",
    "rkeys = list(noise_ts.keys())\n",
    "rkeys.reverse()\n",
    "for i in range(2):\n",
    "    curn = curns[i]\n",
    "    for j in rkeys:\n",
    "        if j == 'noise_0':\n",
    "            a[i].plot(range(noise_ts[j].shape[1]), noise_ts[j][curn,:], linewidth = 6,label=j)\n",
    "        else:\n",
    "            a[i].plot(range(noise_ts[j].shape[1]), noise_ts[j][curn,:],label=j)\n",
    "    a[i].plot(range(noise_ts[j].shape[1]), td_stimtimes_0_5[0], label=\"Task\", color=\"black\")\n",
    "    a[i].legend(loc=\"best\")\n",
    "    a[i].set_ylim([-8,8])\n",
    "    \n",
    "a[0].set_title(\"Stimulated node\")\n",
    "a[1].set_title(\"Non-stimulated node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucr_glms = {}\n",
    "ext_glms = {}\n",
    "\n",
    "for k in noise_ts.keys():\n",
    "    ucr_glms[k] = run_ucr_glm(all_nodes_ts = noise_ts[k], task_reg = td_stimtimes_0_5[0])\n",
    "    ext_glms[k] = run_ext_glm(all_nodes_ts = noise_ts[k], task_reg = td_stimtimes_0_5[0], weight_matrix = base_sim['W'], dt = 1, tau = 1, g = 1, s = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the 'correct' baseline for timeseries that are generated through stimulations both directly from the task as well as the rest of the network we average the magnitude of the signal for timepoints where the task was \"on\" and when the task was \"off\" separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect of adding noise to the timeseries has different effects on the task parameter estimates using the two approaches.  \n",
    "\n",
    "For the classic GLM:\n",
    "- without noise the overestimation for all nodes is the worst\n",
    "- with noise the correct baselines get closer to 0 for stimulated nodes because the mean of the noise distribution is 0. When the SD of the noise distribution is higher when this mean is kept constant then the magnitude of noise added to each time point is larger to keep the average at 0. This leads for the change in activity in each time point to be driven more by noise and less by signal thereby bringing the baseline closer to the mean noise.\n",
    "- with more noise the estimates get closer to their lower true baselines as well, though the overestimation remains a problem throughout.\n",
    "\n",
    "For the extended GLM\n",
    "- without noise the parameter estimates for stimulated nodes are recovered well but the overestimation of the task effect in non-stimulated noides continues to be a problem.\n",
    "- with the addition of noise all nodes are initially overcorrected (though the qualitative difference between stimulated and non-stimulated nodes is preserved) but approach the true baseline with increasing noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [\"C\"+str(x) for x in range(len(noise_ts.keys()))]\n",
    "col_lookup = dict(zip(rkeys, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"][0] = 20\n",
    "plt.rcParams[\"figure.figsize\"][1] = 6\n",
    "fig, a =  plt.subplots(1,2)\n",
    "\n",
    "task_betas = [{k:v['ucr_task_betas'] for k,v in ucr_glms.items()}, {k:v['ext_task_betas'] for k,v in ext_glms.items()}]\n",
    "\n",
    "for i in range(len(task_betas)):\n",
    "    for j in rkeys:\n",
    "        a[i].plot(task_betas[i][j], label = j, color = col_lookup[j])\n",
    "        stim_baseline, nonstim_baseline = get_true_baseline(noise_ts[j])\n",
    "        \n",
    "        a[i].plot(np.concatenate((np.repeat(stim_baseline, 11), np.repeat(nonstim_baseline,94))), \n",
    "                  color = col_lookup[j], linestyle = '--')\n",
    "    \n",
    "        for n in range(1,3):\n",
    "            a[i].axvline(x=35*n,linewidth=2, color='gray', ls = \"--\")\n",
    "        a[i].legend(loc=\"best\")\n",
    "        a[i].set_ylim([-1.5,4.5])\n",
    "        \n",
    "plt.ylabel('Beta',fontsize=14)\n",
    "plt.xlabel('Node',fontsize=14)\n",
    "a[0].set_title(\"Classic GLM\")\n",
    "a[1].set_title(\"Extended GLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better sense of where the task parameter estimates above are coming from we plot the IV (activity of a node) over the DV (task timing) from the models they derived from. The IV's are colored by the amount of noise added to them. Top row depicts the activity of a stimulated node from the hub community while the bottom row is from a nonstimulated node from a local community. The slopes of the dotted lines are the task parameter estimates denoted as single points in the graphs above. The left column plots the raw activity level using the cGLM parameter estimate for the line slopes while the right column plots the residualized activity after the time series is corrected for the activity of the rest of the network.  \n",
    "\n",
    "We observe  \n",
    "- That the timeseries of nodes become more variable with increasing noise levels.  \n",
    "- The slopes for eGLM are lower for both node types compared to cGLM slopes.  \n",
    "- The overcorrection of nonstimulated nodes when a little bit of noise is added is depicted as a negative slope.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"][0] = 20\n",
    "plt.rcParams[\"figure.figsize\"][1] = 12\n",
    "\n",
    "noise_levels = ['noise_1', 'noise_0.5', 'noise_0']\n",
    "plot_nodes = [1, 55]\n",
    "glm_types = [\"ucr\", \"ext\"]\n",
    "\n",
    "clm_var = glm_types\n",
    "row_var = plot_nodes\n",
    "col_var = noise_levels\n",
    "\n",
    "fig, a = plt.subplots(len(clm_var), len(row_var))\n",
    "\n",
    "task_x = td_stimtimes_0_5[0][1:]\n",
    "xvals = np.arange(0,.6,.1)\n",
    "\n",
    "for i in range(len(row_var)):\n",
    "    for j in range(len(clm_var)):\n",
    "        for k in range(len(col_var)):\n",
    "        \n",
    "            cur_row = row_var[i]\n",
    "            cur_clm = clm_var[j]\n",
    "            cur_col = col_var[k]\n",
    "            jit = float(cur_col.split(\"_\")[1])/50\n",
    "            \n",
    "            if cur_clm == \"ucr\":\n",
    "                raw_y = ext_glms[cur_col]['ext_mods'][cur_row].endog\n",
    "                a[i, j].scatter(task_x+jit, raw_y, label = cur_col, alpha=.5)\n",
    "                s = ucr_glms[cur_col]['ucr_task_betas'][cur_row]\n",
    "                y = s*xvals\n",
    "                a[i, j].plot(xvals, y, '--', color = \"C\"+str(k))\n",
    "                a[i, j].set_ylim([-3,4.5])\n",
    "\n",
    "            if cur_clm == \"ext\":\n",
    "                raw_y = ext_glms[cur_col]['ext_mods'][cur_row].endog\n",
    "                res_x = ext_glms[cur_col]['ext_mods'][cur_row].exog[:,:-1]\n",
    "                res_mod = sm.OLS(raw_y, res_x).fit()\n",
    "                res_y = res_mod.resid\n",
    "                a[i, j].scatter(task_x+jit, res_y, label = cur_col, alpha=.5)\n",
    "                s = ext_glms[cur_col]['ext_task_betas'][cur_row]\n",
    "                y = s*xvals\n",
    "                a[i, j].plot(xvals, y, '--', color = \"C\"+str(k))\n",
    "                a[i,j].set_ylim([-3,4.5])\n",
    "\n",
    "            \n",
    "            a[i, j].set_title(\"GLM: \"+ str(cur_clm) +\", Node: \" + str(cur_row))\n",
    "            a[i, j].legend(loc=\"lower center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for the overcorrection of non-stimulated nodes in eGLM with the smallest amount of noise is not immediately apparent. We can understand it better by breaking it down:\n",
    "\n",
    "What is the \"overcorrection\"?   \n",
    "- Negative slope for the dotted line above for non-stimulated nodes (second row right graph above); the dip of eGLM parameter estimates below the baseline minimum.  \n",
    "\n",
    "What is the \"negative slope\"?\n",
    "- Lower mean of time points when task is on compared to when it's off    \n",
    "\n",
    "How is the corrected time series with a little noise different from the corrected timeseries without noise in how it relates to the task regressor? (see below)\n",
    "- Without noise the corrected time series of a non-stimulated node goes down to the same constant (0) level whenever the task is on. So the relationship between the task regressor and the timeseries is completely removed.\n",
    "- With a little bit of noise added to each time point, however, the corrected timeseries of a nonstimulated node hovers around the 0-level, always below the task regressor. Thus, the relationship is not completely removed but systematically but instead becomes negative.\n",
    "- When noise is increase the magnitude of this hovering at each time point is larger, more frequently going above the task regressor. The relationship between the task regressor and the more nosier corrected time series thus returns to 0 on average.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_levels = ['noise_0','noise_0.5','noise_1']\n",
    "plot_node = 55\n",
    "\n",
    "fig, a = plt.subplots(3,2)\n",
    "\n",
    "for i, cur_noise in enumerate(noise_levels):\n",
    "    start = 450\n",
    "    end = 1200\n",
    "    x = range(start, end)\n",
    "    \n",
    "    raw_y = ext_glms[cur_noise]['ext_mods'][plot_node].endog\n",
    "    res_x = ext_glms[cur_noise]['ext_mods'][plot_node].exog[:,:-1]\n",
    "    res_mod = sm.OLS(raw_y, res_x).fit()\n",
    "    res_y = res_mod.resid\n",
    "    \n",
    "    a[i, 0].plot(x, raw_y[start:end],label=\"UCR \"+cur_noise, color = \"C\"+str(i))\n",
    "    a[i, 1].plot(x, res_y[start:end],label=\"EXT \"+cur_noise, color = \"C\"+str(i))\n",
    "    a[i, 0].plot(x, td_stimtimes_0_5[0][start:end], color=\"black\")\n",
    "    a[i, 1].plot(x, td_stimtimes_0_5[0][start:end], color=\"black\")\n",
    "    a[i, 0].legend(loc=\"upper right\")\n",
    "    a[i, 1].legend(loc=\"upper right\")\n",
    "    a[i, 0].set_ylim([-2,2])\n",
    "    a[i, 1].set_ylim([-2,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionalize above plot (even if not for the form it is in above then whatever you'd like it to look like for the rest of tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different network structures\n",
    "\n",
    "A summary of all changing parameters and their effect on task parameters estimated by both methods can be found in the [Summary](#summary) section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing network density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Innetwork density\n",
    "\n",
    "The innetwork density controls the probability of a given node to be connected to another node within the same community. In a topdown task (where only the hub network is stimulated) increasing the innetwork density exacerbates the bleeding of true activity from the stimulated nodes onto non-stimulated nodes in the same network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sim = sim_network_task_glm(innetwork_dsity = .85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sim_network_glm(data = base_sim, alp = .5, ucr_label=\"cGLM (IND = 0.60)\", ext_label=\"eGLM (IND = 0.60)\")\n",
    "plot_sim_network_glm(data = new_sim, ucr_label=\"cGLM (IND = 0.85)\", ext_label=\"eGLM (IND = 0.85)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outnetwork density\n",
    "\n",
    "The outnetwork density controls the probability of local networks having outside connections with other local communities. Since this is a topdown task increasing the outnetwork density doesn't have an effect on neither the classic nor eGLM estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sim = sim_network_task_glm(outnetwork_dsity = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sim_network_glm(data = base_sim, alp = .5, ucr_label=\"cGLM (OND = 0.08)\", ext_label=\"eGLM (OND = 0.08)\")\n",
    "plot_sim_network_glm(data = new_sim, ucr_label=\"cGLM (OND = 0.25)\", ext_label=\"eGLM (OND = 0.25)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hub network density\n",
    "\n",
    "The hub network density controls the probability of a hub node to be connected to other local networks. Increasing it impacts the classic GLM estimates in the local networks by increasing the leakage from the hub network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sim = sim_network_task_glm(hubnetwork_dsity = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sim_network_glm(data = base_sim, alp = .5, ucr_label=\"cGLM (HND = 0.25)\", ext_label=\"eGLM (HND = 0.25)\")\n",
    "plot_sim_network_glm(data = new_sim, ucr_label=\"cGLM (HND = 0.50)\", ext_label=\"eGLM (HND = 0.50)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing number of local communities\n",
    "\n",
    "Does not lead to any difference for either method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sim = sim_network_task_glm(ncommunities=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sim_network_glm(data = base_sim, alp = .5, ucr_label=r\"cGLM (ncoms = 3)\", ext_label=\"eGLM (ncoms = 3)\", ncoms = 4, width = 10)\n",
    "plot_sim_network_glm(data = new_sim, ucr_label=\"cGLM (ncoms = 4)\", ext_label=\"eGLM (ncoms = 4)\", ncoms = 4, width = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing information transfer strength (ITS)\n",
    "\n",
    "The information transfer strength control the degree to which activity incoming from a connected node affects the change in the activity of a given node. One parameter ($g$) controls the degree of change for activity coming from the rest of the network to the node while another ($s$) control the weight of current node's activity for the next time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change in time series\n",
    "\n",
    "Looking at one block of task-on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_sim_g1_s1 = base_sim\n",
    "new_sim_g0_s0 = sim_network_task_glm(s = 0, g = 0)\n",
    "new_sim_g0_s1 = sim_network_task_glm(g = 0, s = 1)\n",
    "new_sim_g1_s0 = sim_network_task_glm(g = 1, s = 0)\n",
    "new_sim_g5_s1 = sim_network_task_glm(g = 5, s = 1)\n",
    "new_sim_g5_s0 = sim_network_task_glm(g = 5, s = 0)\n",
    "new_sim_g1_s5 = sim_network_task_glm(g = 1, s = 5)\n",
    "new_sim_g0_s5 = sim_network_task_glm(g = 0, s = 5)\n",
    "new_sim_g5_s5 = sim_network_task_glm(g = 5, s = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"][0] = 20\n",
    "plt.rcParams[\"figure.figsize\"][1] = 6\n",
    "\n",
    "#stim node, non-stim hub node, non-stim local node\n",
    "plot_nodes = [1, 21, 51]\n",
    "x_len = td_stimtimes_0_5.shape[1]\n",
    "x_len = 1200\n",
    "\n",
    "sims = [new_sim_g0_s0, base_sim, new_sim_g5_s5, new_sim_g1_s0, new_sim_g5_s0, new_sim_g0_s1, new_sim_g0_s5, new_sim_g5_s1, new_sim_g1_s5]\n",
    "\n",
    "titles = ['g, s = 0','g, s = 1', 'g, s = 5', 'g = 1, s = 0', 'g = 5, s = 0', 'g = 0, s = 1', 'g = 0, s = 5', 'g = 5, s = 1', 'g = 1, s = 5']\n",
    "fig, a = plt.subplots(1,3)\n",
    "\n",
    "for i,cur_sim in enumerate(sims):    \n",
    "    for j,cur_node in enumerate(plot_nodes):\n",
    "        a[j].plot(range(x_len), cur_sim['taskdata'][cur_node][:x_len], label = str(titles[i]))\n",
    "        #a[j].plot(range(x_len), td_stimtimes_0_5[0], '--', color = \"black\")\n",
    "        a[j].legend(loc = \"center right\")\n",
    "        a[j].set_title(\"Node: \" +str(cur_node))\n",
    "        a[j].set_ylim([0, 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When both $s$ and $g$ are 0 the time series are identical to (true) task timing for that node.\n",
    "\n",
    "When $s$ is kept constant and $g$ is increased activity is compounded in all nodes ONLY when task is ON. When the global ITS is increased the leakage from stimulated nodes increases. This increases the similarity in the time series of nodes.\n",
    "\n",
    "When $g$ is kept constant and $s$ is increased a qualitative shift happens when $s>1$. When $s\\leq1$ the random activity at the start of the time series diminishes with time. For a stimulated node the activity resulting from stimulation is compounded (to a larger degree than with an increase in $g$ alone) but lack of activity remains the case for nonstimulated nodes (i.e. no leakage since $s$ only affects current node's activity). When $s>1$, however, the random activity in the beginning of the time series is compounded to such a large extent for all nodes that it creates high and spurious (or at least task-unrelated) activity that is also increasingly similar across all nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change in parameter estimates\n",
    "\n",
    "Classic glm on left and extended on right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sims = {\"new_sim_g0_s0\": new_sim_g0_s0, \"new_sim_g1_s1\": base_sim, \"new_sim_g5_s5\": new_sim_g5_s5, \n",
    "#        \"new_sim_g1_s0\": new_sim_g1_s0, \"new_sim_g5_s0\": new_sim_g5_s0, \"new_sim_g0_s1\": new_sim_g0_s1, \n",
    "#        \"new_sim_g0_s5\": new_sim_g0_s5, \"new_sim_g5_s1\": new_sim_g5_s1, \"new_sim_g1_s5\": new_sim_g5_s1}\n",
    "#titles = ['g, s = 0','g, s = 1', 'g, s = 5', 'g = 1, s = 0', 'g = 5, s = 0', 'g = 0, s = 1', 'g = 0, s = 5', 'g = 5, s = 1', 'g = 1, s = 5']\n",
    "\n",
    "sims = {\"new_sim_g0_s0\": new_sim_g0_s0, \"base_sim_g1_s1\": base_sim, \n",
    "        \"new_sim_g1_s0\": new_sim_g1_s0, \"new_sim_g5_s0\": new_sim_g5_s0, \"new_sim_g0_s1\": new_sim_g0_s1, \n",
    "        \"new_sim_g5_s1\": new_sim_g5_s1}\n",
    "\n",
    "values = [\"C\"+str(x) for x in range(len(sims.keys()))]\n",
    "col_lookup = dict(zip(sims.keys(), values))\n",
    "\n",
    "task_betas = [{k:v['ucr_betas'] for k,v in sims.items()}, {k:v['ext_betas'] for k,v in sims.items()}]\n",
    "\n",
    "stim_baselines=[]\n",
    "nonstim_baselines=[]\n",
    "\n",
    "for i in sims.keys():\n",
    "    stim_baseline, nonstim_baseline = get_true_baseline(sims[i]['taskdata'])\n",
    "    stim_baselines.append(stim_baseline)\n",
    "    nonstim_baselines.append(nonstim_baseline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $g=5$ multicollinearity (similarity of each nodes' timeseries to each other) is so bad that the paramater estimates are very variable for both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"][0] = 20\n",
    "plt.rcParams[\"figure.figsize\"][1] = 6\n",
    "fig, a =  plt.subplots(1,2)\n",
    "\n",
    "for i in range(len(task_betas)):\n",
    "    for j, k in enumerate(sims.keys()):\n",
    "        a[i].plot(task_betas[i][k], label = k, color = col_lookup[k])\n",
    "        \n",
    "        a[i].plot(np.concatenate((np.repeat(stim_baselines[j], 11), np.repeat(nonstim_baselines[j],94))), \n",
    "                  color = col_lookup[k], linestyle = '--', alpha=.5)\n",
    "    \n",
    "        for n in range(1,3):\n",
    "            a[i].axvline(x=35*n,linewidth=2, color='gray', ls = \"--\")\n",
    "        a[i].legend(loc=\"best\")\n",
    "        #a[i].set_ylim([-1.5,4.5])\n",
    "        \n",
    "plt.ylabel('Beta',fontsize=14)\n",
    "plt.xlabel('Node',fontsize=14)\n",
    "a[0].set_title(\"Classic GLM\")\n",
    "a[1].set_title(\"Extended GLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What is wrong with the baseline for g=0, s=0?\n",
    "The evoked activity is the same as task timing\n",
    "So the max/only unique number for stimulated timepoints is 0.5\n",
    "The correlation between this and task timing is 1\n",
    "Which is where the cGLM estimates are \n",
    "Does it make sense for the baseline to be at 0.5?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are the estimates for $g = 1, s=0$ identical for both methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sim_network_glm(data = base_sim, alp = .5, ucr_label=\"cGLM (g,s=1)\", ext_label=\"eGLM (g,s=1)\")\n",
    "plot_sim_network_glm(data = new_sim_g1_s0, ucr_label=\"cGLM (g=1, s=0)\", ext_label=\"eGLM (g=1, s=0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing temporal resolution\n",
    "\n",
    "Decrease in $dt$ leads to some improvement of classical GLM estimates of stimulated nodes, while increasing it results in noisier estimates for all nodes.\n",
    "\n",
    "Change in either direction leads to noisier eGLM estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increasing $dt$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sim = sim_network_task_glm(dt=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sim_network_glm(data = base_sim, alp = .5, ucr_label=\"cGLM (dt = 1)\", ext_label=\"eGLM (dt = 1)\")\n",
    "plot_sim_network_glm(data = new_sim, ucr_label=\"cGLM (dt = 2)\", ext_label=\"eGLM (dt = 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decreasing $dt$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sim = sim_network_task_glm(dt = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sim_network_glm(data = base_sim, alp = .5, ucr_label=\"cGLM (dt = 1)\", ext_label=\"eGLM (dt = 1)\")\n",
    "plot_sim_network_glm(data = new_sim, ucr_label=\"cGLM (dt = .5)\", ext_label=\"eGLM (dt = .5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different task structures\n",
    "\n",
    "The initial task was a top down task that stimulated nodes only in the hub network and trickled activity down to the other nodes.\n",
    "\n",
    "Other task activations are possible.\n",
    "\n",
    "Things to modulate:  \n",
    "- Number of nodes stimulated\n",
    "- Stimulating only local community\n",
    "- Stimulating both hub and local community\n",
    "- Magnitude of stimulation (different from 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing number of stimulated nodes\n",
    "\n",
    "Makes the overestimation of non-stimulated node estimates in cGLM and their underestimation in the eGLM slightly worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sim = sim_network_task_glm(stimsize = np.floor(35/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sim_network_glm(data = base_sim, alp = .5, ucr_label=\"cGLM (stim_size=11)\", ext_label=\"eGLM (stim_size=11)\")\n",
    "plot_sim_network_glm(data = new_sim, ucr_label=\"cGLM (stim_size=17)\", ext_label=\"eGLM (stim_size=17)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stimulating only local community\n",
    "\n",
    "The new method seems to work expectedly with a bottom-up task that only stimulates a local community. The overestimated task parameters in the stimulated community are greatly improvement by the eGLM though non-stimulated nodes in the same community are again underestimated. \n",
    "\n",
    "**DOUBLE CHECK LOCAL NODES WITH CONNECTIONS ARE EXTRACTED CORRECTLY**\n",
    "\n",
    "Due to its connections to the stimulated local community there is some leakage to the hub community in the cGLM estimates but again is corrected by eGLM.\n",
    "\n",
    "The second local community's activity is only minimally impacted by the stimulated local community (since any trickling down would come from the hub network). Expectedly both methods do a reasonable job in estimating this lack of stimulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sim = sim_network_task_glm(topdown = False, bottomup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sim_network_glm(data = base_sim, alp = .5, ucr_label=\"cGLM (TD)\", ext_label=\"eGLM (TD)\")\n",
    "plot_sim_network_glm(data = new_sim, ucr_label=\"cGLM (BU)\", ext_label=\"eGLM (BU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stimulating both hub and local community\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sim = sim_network_task_glm(topdown = True, bottomup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sim_network_glm(data = base_sim, alp = .5, ucr_label=\"cGLM (TD)\", ext_label=\"eGLM (TD)\")\n",
    "plot_sim_network_glm(data = new_sim, ucr_label=\"cGLM (TD+BU)\", ext_label=\"eGLM (TD+BU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing magnitude of stimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sim = sim_network_task_glm(stim_mag = .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sim_network_glm(data = base_sim, alp = .5, ucr_label=\"cGLM (stim_mag=0.5)\", ext_label=\"eGLM (stim_mag=0.5)\")\n",
    "plot_sim_network_glm(data = new_sim, ucr_label=\"cGLM (stim_mag=0.8)\", ext_label=\"eGLM (stim_mag=0.8)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "## Summary\n",
    "\n",
    "| Parameters                   | Increase | Decrease |      |\n",
    "|------------------------------|----------|----------|------|\n",
    "| Innetwork density            | -        | +        | cGLM |\n",
    "|                              | o        | o        | eGLM |\n",
    "| Outnetwork density           | o        | o        | cGLM |\n",
    "|                              | o        | o        | eGLM |\n",
    "| Hubnetwork density           | -        | +        | cGLM |\n",
    "|                              | o        | o        | eGLM |\n",
    "| g                            | -        | +        | cGLM |\n",
    "|                              | -        | +        | eGLM |\n",
    "| s                            | -        | +        | cGLM |\n",
    "|                              | -        | +        | eGLM |\n",
    "| tau                          | -        | +        | cGLM |\n",
    "|                              | -        | -        | eGLM |\n",
    "| N local community            | o        | o        | cGLM |\n",
    "|                              | o        | o        | eGLM |\n",
    "| dt                           | +        | -        | cGLM |\n",
    "|                              | -        | -        | eGLM |\n",
    "| N stim nodes                 | -        | +        | cGLM |\n",
    "|                              | -        | +        | eGLM |\n",
    "| bottom up task               |          |          | cGLM |\n",
    "|                              |          |          | eGLM |\n",
    "| top down and  bottom up task | -        |          | cGLM |\n",
    "|                              | -        |          | eGLM |\n",
    "| Magnitude of  stimulation    | o        |          | cGLM |\n",
    "|                              | -        |          | eGLM |"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
